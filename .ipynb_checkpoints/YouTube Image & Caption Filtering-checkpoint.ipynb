{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nehab\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from difflib import SequenceMatcher\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Caption Similarity is Calculated\n",
    "\n",
    "The script calculates caption similarity using Sentence Transformers (BERT-based embeddings). Each caption is converted into a numerical vector representation using the \"all-MiniLM-L6-v2\" model. These embeddings capture the semantic meaning of the text. To compare two captions, the script uses cosine similarity, which measures how close the two vectors are in high-dimensional space. If the similarity score is greater than **0.85**, the captions are considered duplicates, and the older frame may be replaced with the newer one.\n",
    "\n",
    "#### How Image Similarity is Calculated\n",
    "\n",
    "The script calculates image similarity using CLIP embeddings, which encode images into feature vectors that represent their content. Each frame is passed through OpenAI’s CLIP model, generating a vector that describes the image in a way that captures semantic and structural details. The script then uses cosine similarity to compare these vectors—if the similarity is greater than **0.9**, the images are considered visually redundant. In such cases, the older frame is replaced by the newer one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load CLIP model for image similarity\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load Sentence Transformer for caption similarity\n",
    "caption_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_frame_number(frame_name):\n",
    "    \"\"\"Extracts the numerical part of the frame name.\"\"\"\n",
    "    match = re.search(r'frame_(\\d+)', frame_name)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "def extract_video_id_from_filename(filename):\n",
    "    \"\"\"Extracts the YouTube video ID from a filename like 'Ilg3gGewQ5U_processed.json'\"\"\"\n",
    "    match = re.search(r'([0-9A-Za-z_-]{11})_processed', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Extracts CLIP embedding for an image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.get_image_features(**inputs)\n",
    "    return embedding.squeeze().numpy()\n",
    "\n",
    "def get_caption_embedding(caption):\n",
    "    \"\"\"Extracts sentence embedding for a caption using Sentence Transformers.\"\"\"\n",
    "    return caption_model.encode(caption)\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "def filter_frames(json_file, similarity_threshold=0.85, image_threshold=0.9, base_image_folder=\"processed_frames\"):\n",
    "    \"\"\"\n",
    "    Filters frames based on both caption similarity and image similarity.\n",
    "    \"\"\"\n",
    "    video_id = extract_video_id_from_filename(os.path.basename(json_file))\n",
    "    if not video_id:\n",
    "        raise ValueError(\"Could not extract video ID from JSON filename.\")\n",
    "    \n",
    "    image_folder = f\"{base_image_folder}/{video_id}\"\n",
    "    \n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    seen_captions = {}  # Stores caption embeddings\n",
    "    seen_images = {}  # Stores image embeddings\n",
    "    filtered_frames = []  # Stores selected frames\n",
    "\n",
    "    for frame in data:\n",
    "        caption = frame[\"caption\"]\n",
    "        frame_path = f\"{image_folder}/{frame['frame']}\"\n",
    "        frame_number = extract_frame_number(frame[\"frame\"])\n",
    "\n",
    "        # Compute embeddings\n",
    "        caption_embedding = get_caption_embedding(caption)\n",
    "        image_embedding = get_image_embedding(frame_path)\n",
    "\n",
    "        # Check caption similarity\n",
    "        found_similar_caption = None\n",
    "        for seen_caption, seen_caption_embedding in seen_captions.items():\n",
    "            if cosine_sim(caption_embedding, seen_caption_embedding) > similarity_threshold:\n",
    "                found_similar_caption = seen_caption\n",
    "                break\n",
    "\n",
    "        # Check image similarity\n",
    "        found_similar_image = None\n",
    "        for seen_image_path, seen_image_embedding in seen_images.items():\n",
    "            if cosine_sim(image_embedding, seen_image_embedding) > image_threshold:\n",
    "                found_similar_image = seen_image_path\n",
    "                break\n",
    "\n",
    "        if found_similar_caption or found_similar_image:\n",
    "            # If a similar frame exists, replace it only if the new frame is more recent\n",
    "            existing_frame_index = next(\n",
    "                (i for i, f in enumerate(filtered_frames) if f[\"caption\"] == found_similar_caption or f[\"frame\"] == found_similar_image),\n",
    "                None\n",
    "            )\n",
    "\n",
    "            if existing_frame_index is not None and extract_frame_number(filtered_frames[existing_frame_index][\"frame\"]) < frame_number:\n",
    "                filtered_frames[existing_frame_index] = frame  # Replace old frame with newer one\n",
    "                seen_captions[found_similar_caption] = caption_embedding\n",
    "                seen_images[found_similar_image] = image_embedding\n",
    "        else:\n",
    "            filtered_frames.append(frame)\n",
    "            seen_captions[caption] = caption_embedding\n",
    "            seen_images[frame_path] = image_embedding\n",
    "\n",
    "    # Sort frames by their chronological order\n",
    "    filtered_frames.sort(key=lambda x: extract_frame_number(x[\"frame\"]))\n",
    "\n",
    "    # Save the filtered data\n",
    "    output_file = json_file.replace(\".json\", \"_filtered.json\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filtered_frames, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Filtered JSON saved to: {output_file}\")\n",
    "    return filtered_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON saved to: output_data/Ilg3gGewQ5U_processed_filtered.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'frame': 'frame_0.jpg',\n",
       "  'label': 'matchstick',\n",
       "  'caption': 'a black background with a white and red flower',\n",
       "  'extracted_text': ''},\n",
       " {'frame': 'frame_25.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': 'pi rec intuitive walkthon derivative in commatic graphs',\n",
       "  'extracted_text': '* Recap\\n*Intuitive walkthrough\\n\\n* Derivatives in\\ncomputational graphs\\n\\na) = o(2\\\\)\\n\\nCol...) = (a — y)?\\n\\nAL) = wHgE—D 4 yD)\\n\\nDesired\\noutput\\n\\npre\\n\\n'},\n",
       " {'frame': 'frame_50.jpg',\n",
       "  'label': 'breastplate, aegis, egis',\n",
       "  'caption': 'a computer generated image of a red and blue line',\n",
       "  'extracted_text': ''},\n",
       " {'frame': 'frame_70.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': 'what is the cost of difference?',\n",
       "  'extracted_text': 'What’s the “cost”\\n\\n- of this difference?\\n\\nN\\nWSN ZO ELG cS N 0 0\\nSO ew, ge O2\\nRES LO Wes! A 8:\\nSSG NEG er a\\nWoe: PO AESO SIAN ZI O4\\n784 9: Xe KSA Et\\nPINT EER Fe: Os\\nYC ° et\\n\\n'},\n",
       " {'frame': 'frame_75.jpg',\n",
       "  'label': 'scoreboard',\n",
       "  'caption': 'a black background with a white line and a green line with a black background with a white line',\n",
       "  'extracted_text': 'Cost of\\n\\n(0.43 — 0.00)?\\n(0.28 — 0.00)?\\n(0.19 — 0.00)?\\n(0.88 — 1.00)?4\\n(0.72 — 0.00)?\\n(0.01 — 0.00)?\\n(0.64 — 0.00)?\\n(0.86 — 0.00)?\\n(0.99 — 0.00)?\\n(0.63 — 0.00)?\\n\\n+\\n\\nWhat’s the “cost”\\nof this difference?\\n\\nUtter trash\\n'},\n",
       " {'frame': 'frame_120.jpg',\n",
       "  'label': 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
       "  'caption': 'a graph with a number of different numbers',\n",
       "  'extracted_text': '0.16\\n\\n0.72\\n—0.93\\n-vo(...)=| :\\nAll Cts or\\nand biases °\\n1.52\\n\\nC(wo, W1,--- , W13,001) = 2.85\\n\\nDirection in\\n13,002 dimensions?!?\\n'},\n",
       " {'frame': 'frame_175.jpg',\n",
       "  'label': 'menu',\n",
       "  'caption': 'a blackboard with a mathematical equation on it',\n",
       "  'extracted_text': 'ju\\n6! -\\n= (( .\\n; ©)\\n—\\nZz\\noS\\n)\\n©\\na’\\n(2!\\n)\\n'},\n",
       " {'frame': 'frame_190.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': 'pi recative walkthough incorent infinctionaps',\n",
       "  'extracted_text': 'Plan\\n\\nIncrease b\\n\\n*Recap\\n\\ni. Increase w;\\n* Intuitive walkthrough in proportion to a;\\n\\nChange a;\\n\\n¢ Derivatives in\\ncomputational graphs\\n\\n'},\n",
       " {'frame': 'frame_205.jpg',\n",
       "  'label': 'scoreboard',\n",
       "  'caption': 'average and average of a graph',\n",
       "  'extracted_text': '3 Average over all training examples\\n\\nCost of\\none example\\n\\n784\\n\\n'},\n",
       " {'frame': 'frame_235.jpg',\n",
       "  'label': 'Windsor tie',\n",
       "  'caption': 'a black background with a speech bubble and a blue speech bubble',\n",
       "  'extracted_text': 'Bad network!\\n'},\n",
       " {'frame': 'frame_245.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': 'a black background with a white and red line pattern',\n",
       "  'extracted_text': 'You can only adjust weigats ao lt...\\n\\n784 ERIK\\nae\\nLL XS © 6\\n\\niY ON\\nOi / e\\nIZ x \\\\ 7\\n\\ny\\nVa\\n\\n'},\n",
       " {'frame': 'frame_280.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': 'a black background with a white and red dotted line',\n",
       "  'extracted_text': '784\\n\\n'},\n",
       " {'frame': 'frame_325.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': 'a blackboard with a whiteboard and a blackboard with a whiteboard and a whiteboard',\n",
       "  'extracted_text': '= (Wao + WA, Fett FH Wyp—1An—1 + )\\n\\nIncrease b\\n\\nIncrease w;\\n\\nChange a;\\n\\n'},\n",
       " {'frame': 'frame_345.jpg',\n",
       "  'label': 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
       "  'caption': 'a graph graph with a graph graph and a graph graph',\n",
       "  'extracted_text': '0.72\\n—0.93\\n-VCl...)=]\\nAll Cots re\\nand biases °\\n1.52\\n\\nC(wo, W1,--. ,W13,001) = 2.85\\n'},\n",
       " {'frame': 'frame_385.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': 'neurolatregetter wegæer neurolatregetter neurolat',\n",
       "  'extracted_text': '“Neurons that fire together wire together”\\n\\n'},\n",
       " {'frame': 'frame_490.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': 'a black background with a white and yellow text',\n",
       "  'extracted_text': 'Propagate backwards\\n\\nIncrease b\\n\\nIncrease w;\\nin proportion to a;\\n\\nChange a;\\nin proportion to w;\\n\\nOOBOOOOOOOOOOCOO0\\neo eR oOo rr\\n'},\n",
       " {'frame': 'frame_495.jpg',\n",
       "  'label': 'harp',\n",
       "  'caption': 'a diagram of a program with a program in the background',\n",
       "  'extracted_text': 'Propagate backwards\\n\\na\\n\\n@@G000C880080800\\n\\n'},\n",
       " {'frame': 'frame_505.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': 'a diagram of a network with several nodes',\n",
       "  'extracted_text': 'Propagate backwards\\n\\ni\\n\\n@©000000000800800\\n\\ner foley YY Yolel_jel YY lee\\n'},\n",
       " {'frame': 'frame_525.jpg',\n",
       "  'label': 'digital clock',\n",
       "  'caption': 'arabic alphabets - screenshote',\n",
       "  'extracted_text': 'Cee)\\n\\n—0.08\\n\\n—0.11\\n\\n—0.07\\n\\n+0.13\\n\\n'},\n",
       " {'frame': 'frame_535.jpg',\n",
       "  'label': 'digital clock',\n",
       "  'caption': 'a table with numbers and symbols',\n",
       "  'extracted_text': 'Average over\\n\\nall training data\\n\\nWo —0.08 | +0.02 | —0.02 | +0.11 | —0.05 | —0.14\\nWy —~0.11 | +0.11 | +0.07 | +0.02 | +0.09 | +0.05\\nWe —0.07 | —0.04 | —0.01 | +0.02 | +0.13 | —0.15\\nW13,001 | +0.13 | +0.08 | —0.06 | —0.09 | —0.02 | +0.04\\n\\n'},\n",
       " {'frame': 'frame_540.jpg',\n",
       "  'label': 'scoreboard',\n",
       "  'caption': 'a graph with a line of data and a line of data',\n",
       "  'extracted_text': 'Average over\\nall training data\\n\\n\\\\\\n\\n— —0.08\\n\\noO\\nWo —0.08 | +0.02 | —0.02 | +0.11 | —0.05 | —0.14\\nWI\\nW2\\n\\nW13,001\\n\\n'},\n",
       " {'frame': 'frame_555.jpg',\n",
       "  'label': 'digital clock',\n",
       "  'caption': 'a diagram of the algorithm for a algorithm',\n",
       "  'extracted_text': 'Average over\\n\\n‘2 3 | Cy] wate ” a\\n\\n—0.08\\n+0.12\\n\\n—1VC (wy, W2,.-- . W13,001) = | —0.06\\n\\n+ 0.04\\n'},\n",
       " {'frame': 'frame_595.jpg',\n",
       "  'label': 'digital clock',\n",
       "  'caption': 'a black and white font with numbers',\n",
       "  'extracted_text': 'SOU /TANZIN¥ 3 =\\nS617 2 F CF HOPS\\nAZyY,327a F C70\\nGOFF GI FTI 7 8\\n\\n'},\n",
       " {'frame': 'frame_600.jpg',\n",
       "  'label': 'typewriter keyboard',\n",
       "  'caption': 'a set of numbers with the same numbers',\n",
       "  'extracted_text': 'Compute gradient descent step\\n\\naN\\n\\nusing backprop)\\n\\nSIO 7 TA VF I\\nSe1.7AF CSUN OY\\nAZY3 272% °F 670\\n60360187793\\n\\n'},\n",
       " {'frame': 'frame_605.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': 'a keyboard with numbers and numbers on it',\n",
       "  'extracted_text': 'OU STANZA 4H 3\\n\\nG\\\\72 Ff CF MOY\\n\\nCompute gradient descent step (using backprop)\\n\\n60361 8 7\\n\\n293273 F 6 7/0\\n4349 8\\n\\n'},\n",
       " {'frame': 'frame_635.jpg',\n",
       "  'label': 'matchstick',\n",
       "  'caption': 'stochic descent / stochic descent / stochic descent / stochic descent',\n",
       "  'extracted_text': '(/Stochastic gradient descent)\\n'},\n",
       " {'frame': 'frame_640.jpg',\n",
       "  'label': 'hair slide',\n",
       "  'caption': 'the evolution of the evolution of the evolution of the evolution of the evolution of the evolution of the',\n",
       "  'extracted_text': 'Backpropagation\\n\\ntitmt WN\\n'},\n",
       " {'frame': 'frame_650.jpg',\n",
       "  'label': 'fire screen, fireguard',\n",
       "  'caption': 'a black background with a white line and a white line with the words train in progress',\n",
       "  'extracted_text': 'Training in ls\\nprogress. . .\\n\\n784\\n\\n'},\n",
       " {'frame': 'frame_700.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': \"the codel's book\",\n",
       "  'extracted_text': 'def backprop(self, x, y):\\n\\n\"\"\"Return a tuple **(nabla_b, nabla_w)** representing the\\n\\ngradient for the cost function C_x. ~*‘nabla_b** and\\n\\n*“nabla_w** are layer-by-layer lists of numpy arrays, similar\\n\\nto ‘‘self.biases** and ‘*self.weights**.\"\"\"\\n\\nNabla_b = [np.zeros(b.shape) for b in self.biases]\\n\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\n\\n# feedforward\\n\\nactivation = x\\n\\nactivations = [x] # list to store all the activations, layer by layer\\n\\nzs = [] # list to store all the z vectors, layer by layer\\n\\nfor b, w in zip(self.biases, self.weights):\\nz = np.dot(w, activation)+b\\nzs.append(z)\\nactivation = self.non_linearity(z)\\nactivations. append(activation)\\n\\n# backward pass\\n\\ndelta = self.cost_derivative(activations[-1], y) * \\\\\\nself.d_non_linearity(zs[-1])\\n\\nnabla_b[-1] = delta\\n\\nnabla_w[-1] = np.dot(delta, activations [-2].transpose())\\n\\n# Note that the variable l in the loop below is used a little\\n\\n# differently to the notation in Chapter 2 of the book. Here,\\n\\n# 1= 1 means the last layer of neurons, l = 2 is the\\n\\n# second-last layer, and so on. It\\'s a renumbering of the\\n\\n# scheme in the book, used here to take advantage of the fact\\n\\n# that Python can use negative indices in lists.\\n\\nfor lL in xrange(2, self.num_layers):\\nsp si The code you\\'d find\\nsp = self.d_non_linearity(z)\\ndelta = np.dot(self.weights[-1+1].transpose(), delta) * sp € CO € you nh\\nnabla_b[-1] = delta . . )\\nnabla_w[-1] = np.dot(delta, activations [-1-1].transpose()) in Nielsen S book\\n\\nreturn (nabla_b, nabla_w)\\n'},\n",
       " {'frame': 'frame_705.jpg',\n",
       "  'label': 'screwdriver',\n",
       "  'caption': 'a cartoon drawing of a cat and a dog',\n",
       "  'extracted_text': 'wie\\n\\nI...er...can’t follow\\nthat code at all.\\n'},\n",
       " {'frame': 'frame_710.jpg',\n",
       "  'label': 'beaker',\n",
       "  'caption': 'let let let let let let let let let let let let let let let let let let let',\n",
       "  'extracted_text': 'Let’s get to the\\ncalculus then\\n\\ntiHH V\\\\\\n'},\n",
       " {'frame': 'frame_715.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': 'pal rec intuitive walkthough derivative in commatic graphs',\n",
       "  'extracted_text': 'Plan\\n\\nCost —> Co(...) = (a — y)?\\n\\nRecap BD) = whg(E-0 4 po\\nIntuitive walkthrough a = o(2) periieg\\n‘Intuitive walkthroug\\n\\n«  @\\n* Derivatives in A-1) Io 4\\n\\ncomputational graphs\\n\\n'},\n",
       " {'frame': 'frame_730.jpg',\n",
       "  'label': 'digital clock',\n",
       "  'caption': 'a black background with the words mst database and the words mst database',\n",
       "  'extracted_text': 'MNIST Database\\n\\nLeCun, Cortes and Burges\\n\\n(links in the description)\\n\\n1d 4\\n'},\n",
       " {'frame': 'frame_740.jpg',\n",
       "  'label': 'typewriter keyboard',\n",
       "  'caption': 'a set of numbers with blue and white numbers',\n",
       "  'extracted_text': 'iS’ a. ae a a a i ae Ee aS\\n\\na on a nn ~ na Cn ON na on\\n\\na ee ae ae ae a a ae ee ae\\nDON NIN NINN NNN ON\\n\\nna na na nn n~ na nan a na na\\n\\n4 _ 4 _ ae _ 4 _ 4 _ a _ 4 _ a ae\\nLES LOS, LOO LOS LES LON LS SO NN NSN\\n\\nn~ n~ n~ n~ ~ n~ n~ n~ n n~\\n\\na a” a ae ae ae ae ae ae\\nLEO LO, Fs LN LE LES SOL ON FN\\n\\na ~~ na na a a on On na ~\\n\\na ae ae ae ae a ae ae ae ae\\nDON NIN NNN IN NNN\\n\\nnn an na n n na n~ n n nan\\n\\na a a ee ee ee ae ae ae\\nLN NNN NN NN NSN\\n\\nnn n~ n~ nn ~ n~ ~ n~ nn n~\\n\\na 4a” a ae ae ae ae ae ae a\\n, iia i a i NE aan Te i i aia i ailie e anS\\n\\na on a na a a Cn a na on\\n\\na ee ae ae ae a ae ae ae ae\\n\\nfo Oe LO LO LOS, OOS OS OTS\\n\\nna n~ na na a n nan ON na a\\n\\na a ae ae ae ae a ae aE a\\n'},\n",
       " {'frame': 'frame_745.jpg',\n",
       "  'label': 'web site, website, internet site, site',\n",
       "  'caption': 'a diagram of the different types of animals',\n",
       "  'extracted_text': '= —> Lion © — Astrolabe\\n\\nP i-2 Ceni Songbird of\\n‘ys —> Genlus\\nihe our generation\\n\\n—> Fork > Cow\\n\\n'},\n",
       " {'frame': 'frame_750.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': 'a black background with a blue cat on it',\n",
       "  'extracted_text': 'Clicky stuffs\\n\\na\\n'},\n",
       " {'frame': 'frame_760.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': 'a black background with a blue bird on it',\n",
       "  'extracted_text': 'Clicky stuffs\\n\\n'},\n",
       " {'frame': 'frame_765.jpg',\n",
       "  'label': 'analog clock',\n",
       "  'caption': \"a black background with a blue bird and the words'city stuff '\",\n",
       "  'extracted_text': 'Clicky stuffs\\n\\nTU\\n'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "filtered_data = filter_frames(\"output_data/Ilg3gGewQ5U_processed.json\")\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Caption Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load CLIP model for image similarity\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# # Load Sentence Transformer for caption similarity\n",
    "# caption_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# def similar(a, b):\n",
    "#     \"\"\"Computes similarity ratio between two strings using SequenceMatcher.\"\"\"\n",
    "#     return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# def extract_frame_number(frame_name):\n",
    "#     \"\"\"Extracts the numerical part of the frame name.\"\"\"\n",
    "#     match = re.search(r'frame_(\\d+)', frame_name)\n",
    "#     return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "# def get_image_embedding(image_path):\n",
    "#     \"\"\"Extracts CLIP embedding for an image.\"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         embedding = clip_model.get_image_features(**inputs)\n",
    "#     return embedding.squeeze().numpy()\n",
    "\n",
    "# def get_caption_embedding(caption):\n",
    "#     \"\"\"Extracts sentence embedding for a caption using Sentence Transformers.\"\"\"\n",
    "#     return caption_model.encode(caption)\n",
    "\n",
    "# def cosine_sim(vec1, vec2):\n",
    "#     \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "#     return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "# def filter_frames(json_file, similarity_threshold=0.85, image_threshold=0.9, image_folder=\"processed_frames/\"):\n",
    "#     \"\"\"\n",
    "#     Filters frames based on both caption similarity and image similarity.\n",
    "#     \"\"\"\n",
    "#     with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     seen_captions = {}\n",
    "#     seen_images = {}\n",
    "\n",
    "#     for frame in data:\n",
    "#         caption = frame[\"caption\"]\n",
    "#         frame_path = f\"{image_folder}/{frame['frame']}\"\n",
    "#         frame_number = extract_frame_number(frame[\"frame\"])\n",
    "\n",
    "#         # Compute embeddings\n",
    "#         caption_embedding = get_caption_embedding(caption)\n",
    "#         image_embedding = get_image_embedding(frame_path)\n",
    "\n",
    "#         # Check caption similarity\n",
    "#         found_similar_caption = None\n",
    "#         for seen_caption, seen_caption_embedding in seen_captions.items():\n",
    "#             if cosine_sim(caption_embedding, seen_caption_embedding) > similarity_threshold:\n",
    "#                 found_similar_caption = seen_caption\n",
    "#                 break\n",
    "\n",
    "#         # Check image similarity\n",
    "#         found_similar_image = None\n",
    "#         for seen_image_path, seen_image_embedding in seen_images.items():\n",
    "#             if cosine_sim(image_embedding, seen_image_embedding) > image_threshold:\n",
    "#                 found_similar_image = seen_image_path\n",
    "#                 break\n",
    "\n",
    "#         if found_similar_caption or found_similar_image:\n",
    "#             # If a similar frame exists, replace it only if the new frame is more recent\n",
    "#             existing_frame = seen_captions.get(found_similar_caption, seen_images.get(found_similar_image))\n",
    "#             if existing_frame and extract_frame_number(existing_frame[\"frame\"]) < frame_number:\n",
    "#                 if found_similar_caption:\n",
    "#                     seen_captions[found_similar_caption] = frame\n",
    "#                 if found_similar_image:\n",
    "#                     seen_images[found_similar_image] = image_embedding\n",
    "#         else:\n",
    "#             seen_captions[caption] = frame\n",
    "#             seen_images[frame_path] = image_embedding\n",
    "\n",
    "#     # Sort frames by their chronological order\n",
    "#     filtered_frames = sorted(seen_captions.values(), key=lambda x: extract_frame_number(x[\"frame\"]))\n",
    "\n",
    "#     # Save the filtered data\n",
    "#     output_file = json_file.replace(\".json\", \"_filtered.json\")\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(filtered_frames, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "#     print(f\"Filtered JSON saved to: {output_file}\")\n",
    "#     return filtered_frames\n",
    "\n",
    "# # Example usage:\n",
    "# filtered_data = filter_frames(\"output_data/Ilg3gGewQ5U_processed.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# filtered_data = filter_frames(\"output_data/Ilg3gGewQ5U_processed.json\")\n",
    "# filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
