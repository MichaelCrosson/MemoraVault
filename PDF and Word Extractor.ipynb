{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2\n",
    "#!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2 #type: ignore\n",
    "import docx #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    # Replace all whitespace (including newlines) with a single space\n",
    "                    cleaned_text = \" \".join(page_text.split())\n",
    "                    text += cleaned_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_word(doc_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = docx.Document(doc_path)\n",
    "        text = \"\\n\".join(para.text for para in doc.paragraphs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {doc_path}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Process PDFs\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            print(f\"Processing PDF: {filename}\")\n",
    "            pdf_text = extract_text_from_pdf(file_path)\n",
    "            # Here you can add your code to pass pdf_text through your RAG pipeline\n",
    "            print(pdf_text)\n",
    "        \n",
    "        # Process Word documents\n",
    "        elif filename.lower().endswith('.docx'):\n",
    "            print(f\"Processing Word Document: {filename}\")\n",
    "            word_text = extract_text_from_word(file_path)\n",
    "            # Here you can add your code to pass word_text through your RAG pipeline\n",
    "            print(word_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Word Document: AML Quiz 1 Study Guide.docx\n",
      "3 Types of Analytics\n",
      "Descriptive - find human interpretable patterns that describe the data\n",
      "Large scale summaries, local patterns - finding fraud\n",
      "Predictive - use variables to predict unknown or future values of other variables\n",
      "Regression, classification, ranking/recommendation\n",
      "Prescriptive (may need causal reasoning)\n",
      "\n",
      "Analysis is often retrospective (not prospective) data was not collected in a methodical way that is tailored for the analytical task\n",
      "Data may be collected in the past without a specific analytical task in mind, which can lead to challenges because the data was not gathered with a methodology to ensure relevance or completeness for the analytics to be done - more risk for biases that are not controlled\n",
      "\n",
      "No Free Lunch - there is no universally best model, so tradeoffs need to be understood\n",
      "Deep nets and complex models are very general and powerful (few to no assumptions about the nature of relationships) but require lots of data, hyperparameter optimization, compute, little statistical or human insights, and the solution may not be robust\n",
      "\n",
      "Sanity checks are crucial to make sure that model’s assumptions are correct, preventing overfitting, ensuring integrity of the model\n",
      "\n",
      "Confounders\n",
      "\n",
      "Here, the confounding variable is temperature: high temperatures cause people to both eat more ice cream and spend more time outdoors under the sun, resulting in more sunburns.\n",
      "Confounders are correlated with the independent variable (may be causation) and causally related to the dependent variable\n",
      "Confounders bias results and prevent you from seeing actual relationships\n",
      "\n",
      "Bayes Rule\n",
      "\n",
      "Prior probability = p(Y)\n",
      "Probability of case without information\n",
      "Posterior probability = p(Y|X)\n",
      "Probability of case taking into account new information\n",
      "\n",
      "Independence\n",
      "If p(X,Y) = p(x)*p(y) or p(Y|X) = p(Y)\n",
      "\n",
      "Likelihood Function\n",
      "Maximum likelihood is an approach for determining the parameters in a probability distribution using an observed data set. Approach is to find parameter values that maximize the likelihood function\n",
      "We maximize the log of a function because it's more convenient - more mathematically simple and because the product of a large number of small probabilities can underflow the numerical precision of the computer but the sum of log probabilities wouldn't\n",
      "First maximize mean and then variance\n",
      "Over a sufficient data set the mean should equal the true mean but the variance estimate will underestimate by a factor of (N-1)/N - this is due to us maximizing the mean and then the variance, creating bias\n",
      "Becomes less severe with higher N, but more severe with more parameters\n",
      "Negative log likelihood can be interpreted as a cost function similar to MSE for estimating the parameters of the distribution\n",
      "Likelihood refers to when you are trying to find the optimal value for the mean or standard deviation for a distribution given a bunch of observed measurements\n",
      "Probability is the likelihood of observing measurements given a distribution\n",
      "\n",
      "Marginal distribution\n",
      "Probability distribution of a subset from a larger set of variables\n",
      "Represents probabilities of the subset while ignoring or summing over the probabilities of the other variables\n",
      "\n",
      "Conditional Distribution\n",
      "Distribution of one variable give something true about the other variable\n",
      "Usually in % - % of people who have a gpa Y given they are age 20\n",
      "\n",
      "Covariance\n",
      "Statistical measure that indicates the degree to which 2 random variables change together, does not provide information on the strength of the relationship (not normalized like correlation)\n",
      "\n",
      "Multiple Linear Regression - MLR\n",
      "Statistical format = \n",
      "Coefficients are betas, dependent variable is Y, estimates are indicated by hats\n",
      "Learned by Ordinary Least Squares OLS - hyperplane is fitted by minimizing the mean squared error MSE\n",
      "Alternative statistical format\n",
      "\n",
      "Meaning: Y|Dependents ~ Error terms\n",
      "Y|Dependents: Conditional mean of Y is linear in the dependent variables\n",
      "Error term:\n",
      "Normally distributed\n",
      "Independent from each other\n",
      "Identically distributed\n",
      "Minimizing MSE on the training data yields the maximum likelihood estimate (MLE) solution of the assumed generative model\n",
      "ML Notation (Generalized linear regression)\n",
      "Generalized linear regression is a linear combination of basis functions\n",
      "\n",
      "y(x,w) is the output (dependent)\n",
      "W is the vector of weights of coefficients\n",
      "ϕi(x) are the basis functions which transform the input x\n",
      "M represents the number of basis functions \n",
      "Parametric model - Assume a predetermined structure and a set of parameters(weights) to estimate using data\n",
      "Steps for learning a parametric model:\n",
      "Determine the functional form of the model (polynomials, neural networks, linear functions) which are the mathematical structure that describes how the inputs relate to the outputs\n",
      "Learn the parameters (weights) using the training data. In linear models, parameters would be the coefficients/betas\n",
      "Special cases\n",
      "Linear regression - most basic case where the basis function ϕi(x) is simply the input variables themselves - no transformation\n",
      "Polynomial regression with scalar x - in this case the input x has polynomial terms (x, x2,x3) so the MLR model uses ϕi(x) = xi which allows the model to fit curves to the data\n",
      "Interpreting betas\n",
      "\n",
      "Beta is the partial derivative of the expected value of Y with respect to the predictor Xj, holding all other predictors constant - Bj is how much Y is expected to change for a one-unit change in Xj all other variables fixed\n",
      "\n",
      "Collinearity\n",
      "Collinearity is a situation where 2 or more predictor variables in a multiple regression model are highly correlated\n",
      "Mostly effects interpretability, does not affect MSE or R^2\n",
      "When there is collinearity the predictors are not independent, which causes the coefficients to be less interpretable - changes in one variable are associated with the other which makes it hard to isolate the effect of individual predictors\n",
      "Increases uncertainty in coefficient estimates - small changes in the data can then lead to large fluctuations in the estimated values of the coefficients\n",
      "Dummy variables/one hot encoding\n",
      "Must always do 1 less than the total amount\n",
      "If you do create dummies for each category then you have perfect multicollinearity - sum of all dummies will always equal 1 for each observation. This linear dependency makes the model matrix singular, therefore the regression algorithm cannot invert the matrix to estimate the parameters.\n",
      "\n",
      "Ordinary Least Squares (OLS)\n",
      "Method used to estimate the parameters/weights of a regression model by minimizing the sum of squared errors (SSE)\n",
      "Loss function E(w) is the loss function OLS tries to minimize\n",
      "\n",
      "½ is for mathematical simplicity because you take the derivative and the squared cancels it out\n",
      "Minimizing MSE/SSE on the training data yields the Maximum likelihood estimate solution\n",
      "Under the assumptions that the errors are normally distributed with constant variance\n",
      "\n",
      "(Batch Mode) Least Squares Solution\n",
      "\n",
      "Method for finding the optimal parameters (w) in a linear regression by using an exact closed form solution\n",
      "Uses matrix inversion on an (M+1) x (M+1) matrix where M is the number of basis functions (features). Φ0x = 1 which corresponds to the intercepts, where w0 is the coefficient for the intercept\n",
      "Can become computationally demanding as M increases\n",
      "Computation is linear in the size of the dataset (N) but cubic in the number of features (M) - more manageable to scale with more data points than with more features\n",
      "Method is batch mode which means it process the entire dataset at once to calculate the solution which is efficient for small or medium datasets but may not be for larger ones\n",
      "Collinearity problem is explicit because collinear variables makes the matrix difficult to invert, leading to high uncertainties in the parameter estimates\n",
      "\n",
      "Linear Least Squares vs Total Least Squares\n",
      "\n",
      "Linear least squares is used by OLS and minimizes the vertical distances between the observed data and regression line - only accounts for errors in the dependent variable and assumes the predictors are measured without error\n",
      "Best for when error or noise is only found in the dependent variable\n",
      "Total least squares minimizes perpendicular distances from the data points to the regression line, which accounts for errors in both dependent and independent variables\n",
      "Best for when there are measurement errors in both independent and dependent\n",
      "\n",
      "Model Complexity and Overfitting\n",
      "High complexity (more features) increases risk of overfitting, high N (more data points) reduces risk of overfitting with more complex models\n",
      "Regularization - impose penalties on less desirable solutions\n",
      "\n",
      "Lambda is alpha in SKlearn\n",
      "Regularization penalty (Penalty(f)) is a functional (maps the function of the regularization onto a number) which quantifies how complex or undesirable the solution is\n",
      "Intercept is not penalized\n",
      "Variables should be standardized so all features are on the same scale/magnitude otherwise features with large scale would dominate the penalty term\n",
      "Popular penalties\n",
      "Ridge Regression - penalty is sum of squared weights\n",
      "Discourages large weight values and shrinks them toward zero (never exactly zero) - also known as shrinkage or weight decay. Useful when we want to keep all features\n",
      "Ridge regression has an exact closed form solution\n",
      "Lasso Regression - adds penalty on the sum of the absolute values of the weights. \n",
      "Can push some weights to exactly zero which performs feature selection\n",
      "Elastic Net - combines both ridge and lasso penalties by shrinking weights and setting some weights to zero\n",
      "Number of non-zero weights - leads weights to be driven to zero\n",
      "Smoothness of function - penalize models that are not smooth (punishes I’m fluctuation)\n",
      "Kaggle data set issue\n",
      "Test set may be different if you sample the data again, meaning other models may perform better on other test sets - want to find how good the data would be on an infinite test set (true generalization error)\n",
      "\n",
      "Adjusted R^2\n",
      "Adjusts for the difference between your training data and true data - so as the amount of training data increases, the difference can be expected to shrink\n",
      "Processing PDF: AML Quiz 1 Study Guide.pdf\n",
      "3 Types of Analytics 1. Descriptive - find human interpretable patterns that describe the data a. Large scale summaries, local patterns - finding fraud 2. Predictive - use variables to predict unknown or future values of other variables a. Regression, classification, ranking/recommendation 3. Prescriptive (may need causal reasoning) Analysis is often retrospective (not prospective) data was not collected in a methodical way that is tailored for the analytical task ● Data may be collected in the past without a specific analytical task in mind, which can lead to challenges because the data was not gathered with a methodology to ensure relevance or completeness for the analytics to be done - more risk for biases that are not controlled No Free Lunch - there is no universally best model, so tradeoffs need to be understood ● Deep nets and complex models are very general and powerful (few to no assumptions about the nature of relationships) but require lots of data, hyperparameter optimization, compute, little statistical or human insights, and the solution may not be robust Sanity checks are crucial to make sure that model’s assumptions are correct, preventing overfitting, ensuring integrity of the model Confounders ● Here, the confounding variable is temperature: high temperatures cause people to both eat more ice cream and spend more time outdoors under the sun, resulting in more sunburns. ● Confounders are correlated with the independent variable (may be causation) and causally related to the dependent variable\n",
      "● Confounders bias results and prevent you from seeing actual relationships Bayes Rule ● Prior probability = p(Y) ○ Probability of case without information ● Posterior probability = p(Y|X) ○ Probability of case taking into account new information Independence ● If p(X,Y) = p(x)*p(y) or p(Y|X) = p(Y) Likelihood Function ● Maximum likelihood is an approach for determining the parameters in a probability distribution using an observed data set. Approach is to find parameter values that maximize the likelihood function ● We maximize the log of a function because it's more convenient - more mathematically simple and because the product of a large number of small probabilities can underflow the numerical precision of the computer but the sum of log probabilities wouldn't ● First maximize mean and then variance ● Over a sufficient data set the mean should equal the true mean but the variance estimate will underestimate by a factor of (N-1)/N - this is due to us maximizing the mean and then the variance, creating bias ○ Becomes less severe with higher N, but more severe with more parameters ● Negative log likelihood can be interpreted as a cost function similar to MSE for estimating the parameters of the distribution ● Likelihood refers to when you are trying to find the optimal value for the mean or standard deviation for a distribution given a bunch of observed measurements ○ Probability is the likelihood of observing measurements given a distribution Marginal distribution ● Probability distribution of a subset from a larger set of variables ○ Represents probabilities of the subset while ignoring or summing over the probabilities of the other variables Conditional Distribution ● Distribution of one variable give something true about the other variable ○ Usually in % - % of people who have a gpa Y given they are age 20 Covariance\n",
      "● Statistical measure that indicates the degree to which 2 random variables change together, does not provide information on the strength of the relationship (not normalized like correlation) Multiple Linear Regression - MLR ● Statistical format = ○ Coefficients are betas, dependent variable is Y, estimates are indicated by hats ○ Learned by Ordinary Least Squares OLS - hyperplane is fitted by minimizing the mean squared error MSE ● Alternative statistical format ○ ○ Meaning: Y|Dependents ~ Error terms ■ Y|Dependents: Conditional mean of Y is linear in the dependent variables ■ Error term: ● Normally distributed ● Independent from each other ● Identically distributed ○ Minimizing MSE on the training data yields the maximum likelihood estimate (MLE) solution of the assumed generative model ● ML Notation (Generalized linear regression) ○ Generalized linear regression is a linear combination of basis functions ■ y(x,w) is the output (dependent) ■ W is the vector of weights of coefficients ■ ϕ i (x) are the basis functions which transform the input x ■ M represents the number of basis functions ○ Parametric model - Assume a predetermined structure and a set of parameters(weights) to estimate using data ○ Steps for learning a parametric model: ■ Determine the functional form of the model (polynomials, neural networks, linear functions) which are the mathematical structure that describes how the inputs relate to the outputs ■ Learn the parameters (weights) using the training data. In linear models, parameters would be the coefficients/betas\n",
      "○ Special cases ■ Linear regression - most basic case where the basis function ϕ i (x) is simply the input variables themselves - no transformation ■ Polynomial regression with scalar x - in this case the input x has polynomial terms (x, x 2 ,x 3 ) so the MLR model uses ϕ i (x) = x i which allows the model to fit curves to the data ● Interpreting betas ○ Beta is the partial derivative of the expected value of Y with respect to the predictor X j , holding all other predictors constant - B j is how much Y is expected to change for a one-unit change in X j all other variables fixed Collinearity ● Collinearity is a situation where 2 or more predictor variables in a multiple regression model are highly correlated ● Mostly effects interpretability, does not affect MSE or R^2 ○ When there is collinearity the predictors are not independent, which causes the coefficients to be less interpretable - changes in one variable are associated with the other which makes it hard to isolate the effect of individual predictors ● Increases uncertainty in coefficient estimates - small changes in the data can then lead to large fluctuations in the estimated values of the coefficients ● Dummy variables/one hot encoding ○ Must always do 1 less than the total amount ○ If you do create dummies for each category then you have perfect multicollinearity - sum of all dummies will always equal 1 for each observation. This linear dependency makes the model matrix singular , therefore the regression algorithm cannot invert the matrix to estimate the parameters. Ordinary Least Squares (OLS) ● Method used to estimate the parameters/weights of a regression model by minimizing the sum of squared errors (SSE)\n",
      "● Loss function E(w) is the loss function OLS tries to minimize ○ ½ is for mathematical simplicity because you take the derivative and the squared cancels it out ● Minimizing MSE/SSE on the training data yields the Maximum likelihood estimate solution ○ Under the assumptions that the errors are normally distributed with constant variance (Batch Mode) Least Squares Solution ● Method for finding the optimal parameters (w) in a linear regression by using an exact closed form solution ● Uses matrix inversion on an (M+1) x (M+1) matrix where M is the number of basis functions (features). Φ 0 x = 1 which corresponds to the intercepts, where w 0 is the coefficient for the intercept ○ Can become computationally demanding as M increases ● Computation is linear in the size of the dataset (N) but cubic in the number of features (M) - more manageable to scale with more data points than with more features ● Method is batch mode which means it process the entire dataset at once to calculate the solution which is efficient for small or medium datasets but may not be for larger ones ● Collinearity problem is explicit because collinear variables makes the matrix difficult to invert, leading to high uncertainties in the parameter estimates Linear Least Squares vs Total Least Squares\n",
      "● Linear least squares is used by OLS and minimizes the vertical distances between the observed data and regression line - only accounts for errors in the dependent variable and assumes the predictors are measured without error ○ Best for when error or noise is only found in the dependent variable ● Total least squares minimizes perpendicular distances from the data points to the regression line, which accounts for errors in both dependent and independent variables ○ Best for when there are measurement errors in both independent and dependent Model Complexity and Overfitting ● High complexity (more features) increases risk of overfitting, high N (more data points) reduces risk of overfitting with more complex models ● Regularization - impose penalties on less desirable solutions ○ Lambda is alpha in SKlearn ○ Regularization penalty (Penalty(f)) is a functional (maps the function of the regularization onto a number) which quantifies how complex or undesirable the solution is ○ Intercept is not penalized\n",
      "○ Variables should be standardized so all features are on the same scale/magnitude otherwise features with large scale would dominate the penalty term ● Popular penalties ○ Ridge Regression - penalty is sum of squared weights ■ Discourages large weight values and shrinks them toward zero (never exactly zero) - also known as shrinkage or weight decay. Useful when we want to keep all features ■ Ridge regression has an exact closed form solution ○ Lasso Regression - adds penalty on the sum of the absolute values of the weights . ■ Can push some weights to exactly zero which performs feature selection ○ Elastic Net - combines both ridge and lasso penalties by shrinking weights and setting some weights to zero ○ Number of non-zero weights - leads weights to be driven to zero ○ Smoothness of function - penalize models that are not smooth (punishes I’m fluctuation) Kaggle data set issue ● Test set may be different if you sample the data again, meaning other models may perform better on other test sets - want to find how good the data would be on an infinite test set (true generalization error) Adjusted R^2 ● Adjusts for the difference between your training data and true data - so as the amount of training data increases, the difference can be expected to shrink\n",
      "\n",
      "Processing Word Document: Exam Review Sheet_ Ethics of AI.docx\n",
      "Exam 1\n",
      "\n",
      "Artifacts and Abstraction (Sessions 0 - 2)\n",
      "\n",
      "There are two ways in which artifacts can contain political properties, where we define political as an arrangement of power and authority in human association as well as the activities that take place within those arrangements:\n",
      "\n",
      "Technical arrangements as a form of order\n",
      "Precedes use of the actual thing\n",
      "Intended\n",
      "Can also transcend simple category of “intended” and “unintended”\n",
      "Example: Low hanging overpasses on Long Island designed to enforce social segregation; blocks off beach access to low-income people\n",
      "Example: Park benches and homeless; no handrests makes it harder for them to lie down on it.\n",
      "Inherently political technologies\n",
      "Adoption of a certain technology requires or is strongly compatible with certain social conditions.\n",
      "Example: Nuclear power plants encourage centralized, hierarchical control structures due to the complexity and potential hazards associated with nuclear technology. This need for centralized control and specialized expertise leads to a form of social organization that is highly structured and authoritarian, reflecting and reinforcing certain power dynamics within society.\n",
      "Example: Cotton mills; people had to work a very specific way and in a certain timeframe; sequence is super important (do exactly what you are told to do)\n",
      "\n",
      "False Example: Energy sources have politics because the impact they have on climate change is an inherently political property. \n",
      "\n",
      "Main AI Ingredient = Data\n",
      "\n",
      "We define AI as the  “use of technologies to build machines and computers that have the ability to mimic cognitive functions associated with human intelligence.”\n",
      "\n",
      "We define ML as a subset of AI, and it is the “use of algorithms to automatically learn insights and recognize patterns from data.”\n",
      "\n",
      "We define supervised learning as a subset of ML, and it is the “use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.”\n",
      "\n",
      "Values Embedded in ML Pipeline (Sessions 3 - 6)\n",
      "\n",
      "Basics of supervised learning: We take input data, X, and using a function f(X), predict known labels,Y. \n",
      "\n",
      "Machine Learning Pipeline: \n",
      "Problem formulation\n",
      "What is the overarching goal of the system?\n",
      "Different stakeholders can have different goals. \n",
      "What is the mechanism of entry into the population subjected to the algorithm?\n",
      "Need to consider who will encounter the algorithm and if they have a choice.\n",
      "Not determined by the sampling approach used to collect training data! It is who will encounter the algorithm! \n",
      "What is the space of possible decisions?\n",
      "Need to consider the decisions the algorithm is informing and alternative decision spaces. This refers to the different decisions that could be made with the algorithm. \n",
      "Algorithms that solve seemingly the same task can be embedded within entirely different problem formulations, which directly impacts fairness considerations.\n",
      "Need to consider what burdens and benefits are being allocated. \n",
      "Some tasks may be inherently biased and grounded on discriminatory assumptions.\n",
      "It can be misleading to talk about “fairness”; can be narrowly defined as disparities in performance.\n",
      "Accuracy and good AI performance can also be harmful!\n",
      "Data collection and representation\n",
      "Garbage in = Garbage out.\n",
      "ML algorithms are often trained with convenient, inexpensive data.\n",
      "Big data ≠ Good data.\n",
      "Is the data collection ethical?\n",
      "Need to consider if the people that are represented have given consent, and if that consent is meaningful.\n",
      "People can be mislead into giving consent or have no alternative.\n",
      "The data market may be noxious in that our personal data may be commoditized (along with our privacy right), which threatens equal fundamental political rights and freedoms, and equal rights to a fair share of social welfare and equality of opportunity; morally objectionable; harmful outcomes to individuals or society. \n",
      "Sampling Bias: Who is represented?\n",
      "Occurs when the distribution of the sampled data does not match the distribution of the population that will encounter the algorithm\n",
      "This matters because the resulting bias can have severe consequences, such as in responding to disasters (among other tasks).\n",
      "Sources:\n",
      "Access to technology and resources.\n",
      "Previously served/underserved communities.\n",
      "Trust in authorities (reporting).\n",
      "Invisibilization in sampling bias often compounds previous injustices.\n",
      "Differential Subgroup Validity: How are people represented? \n",
      "Predictive power of features collected may differ across subpopulations. \n",
      "Example: In healthcare, symptoms that are studied, taught, and recorded may only hold diagnostic power for some (skintones). \n",
      "Choice may be informed by, and only hold predictive power, in some cultural contexts.\n",
      "Number of credit cards as a positive signal for “creditworthiness.” \n",
      "Ability to adapt to a certain choice of features may also differ across groups. Moreover, strategic adaptation to incentives is not possible for everyone.\n",
      "Example: standardized tests - we can incentivize students to invest in tutoring and retake tests, but not everyone can do this.\n",
      "Can we measure what we care about: mind the gap\n",
      "Y → Quantifiable, imperfect proxy (substitute for ideal target variable)\n",
      "Yc → Complex, multi-faceted (and sometimes contested) outcomes; what we aim for, but even this can be debated (think of capstone)\n",
      "Misleading comparisons: \n",
      "Human vs Machine is not necessarily a fair comparison; calling one of them “better” can be misleading because it depends on the context; a human and machine may not engage in the same predictive task; it depends on context.\n",
      "Self-fulfilling prophecy: When you assume the proxy is a good target variable substitute, this effect overall outcome prediction evaluations\n",
      "Human assessments encoded as labels:\n",
      "Target labels are often termed “ground truth”, but they may also encode biases. We have to consider whose views are encoded and valued when humans determine the target labels.\n",
      "Example Question: An organization is developing a disaster relief system that can support rescue efforts during natural disasters. The algorithm identifies anomalous patters in social media activity. However, a team inside the organization is concerned that this tool may not provide equal support to all communities. The salient type of bias that this is likely to affect this system is: sampling bias. \n",
      "Optimization objectives and evaluation (can go back to 2)\n",
      "How do we define good? What does “good performance” mean for an AI system? What is the optimization objective? How do we evaluate performance? We need to consider all of these question. \n",
      "Optimizing for overall performance is not ideal because it does not implicitly optimize performance for all subgroups. It can actually lead to disparate performance across subgroups.\n",
      "Common predictive performance metric: ROC AUC (Area under the ROC Curve).\n",
      "There is no one-size-fits-all metric of success:\n",
      "In healthcare, we often care about performance at low false negative rates (diabetes example) as opposed to fasle positive rates. This differs for other business contexts. \n",
      "In many cases accuracy does not capture our business needs. \n",
      "Example: A third party platform like Zillow, who could be making an algorithm to price houses, has to maintain both sides between buyer and the seller. Balancing between overestimating and underestimating in predictions can be tricky, and accuracy does not capture this balance. \n",
      "Accuracy can be misleading. \n",
      "Example: For a system with 90% accuracy, it can be useless if you classify everything as one class and this accuracy is a result of the data distribution. \n",
      "In ML we often take for granted that “overall performance” is the desired goal. Is something that works “for most” enough? We do not think so for many instances in our society.\n",
      "Example: Stairs work for 90% of people, but what about the 10% in wheelchairs?\n",
      "Algorithmic adoption (can go back to 2 or 1)\n",
      "Incentives for deployment (and overarching goals); need to consider the following\n",
      "How is a certain technology integrated into a broader social technical system?\n",
      "In the case of AI-assisted decisions, how are algorithmic recommendations integrated into decisions?\n",
      "Ethical implications for future of work; need to consider the following:\n",
      "Who gains from this efficiency? Who owns the labor?\n",
      "Different possibilities under different social/power organizations?\n",
      "Are there trade-offs?\n",
      "Efficiency at what cost?\n",
      "How do algorithmic recommendations interact with human beliefs, values, and abilities?; need to consider the following\n",
      "AI technologies may exacerbate ethical concerns in a socialtechnical system (and humans may exacerbate ethical concerns of algorithm)\n",
      "Feedback loops and algorithmic manipulation:\n",
      "Once deployed, algorithms are actors that alter the state of the world\n",
      "There are two key ways in which this affects the ethical properties of the algorithm:\n",
      "Algorithms can manipulate us: is an algorithm that predicts what you are likely to buy addressing your needs/wants or is it altering what you think you want?\n",
      "Once deployed, algorithms create incentives; people and organizations may adapt to it\n",
      "Different people may have different resources/abilities to adapt (recall standardized tests)\n",
      "Goodhart’s Law: When a measure becomes a target, it ceases to be a good measure. \n",
      "In other words, when we use a measure to reward performance, we provide an incentive to manipulate the measure in order to receive the reward.\n",
      "\n",
      "Algorithmic Bias Detection (Sessions 7 - 9)\n",
      "\n",
      "Algorithmic Bias:\n",
      "One concrete type of harm that may stem from the use of ML\n",
      "Defined as algorithmic outputs that may result in disparate outcomes or other forms of injustices for subgroups of the population, especially those who have been historically marginalized\n",
      "Important on the basis of legal compliance, social responsibility, and utility\n",
      "\n",
      "Why is discrimination wrong?\n",
      "Argument can be made that the whole point of using ML algorithms to discriminate\n",
      "Systematic relative disadvantage: treatment that systematically imposes a disadvantage on one social group relative to others.\n",
      "Different normative underpinning for why it is wrong:\n",
      "Relevance: Relying on characteristics that bear little or no relevance to the outcome or quality that decision makers might be trying to predict or assess\n",
      "Generalizations: Needlessly coarse groupings\n",
      "Is there additional info that could provide a more granular view?\n",
      "Example: Provide a fitness test to all firefighter applicants instead of banning women.\n",
      "Prejudice: When decision makers hold entire groups in lesser regard than others\n",
      "Disrespect: Casting certain groups as categorically inferior to others and thus not worthy of equal respect\n",
      "Different from prejudice because it is about the message that the actions of the decision makers is sending.\n",
      "Immutability: Treating people differently according to characteristics over which they possess no control.\n",
      "Compounding injustice: People cannot be morally culpable for certain facts about themselves that are not the result of their own actions.\n",
      "This is especially the case if these facts are the result of some past injustice.\n",
      "\n",
      "Can it be unjust if it is accurate?\n",
      "It may depend on your normative grounding.\n",
      "Grounded on a compounding injustice view, there may be non-accuracy affecting injustices:\n",
      "Accuracy-affecting injustices: Either the data or output of a model inaccurately estimate a fact about people. (Accuracy is inaccurate)\n",
      "EX: Gaps in FP or FN Rates - emphasis on whether prediction is correct or rate we are incurring errors\n",
      "Non-accuracy-affecting injustices: Data and models accurately estimate a fact about people, but these traits themselves result from injustice. \n",
      "EX: Demographic Parity (gives us different grounds for why we want a certain rate to be the same; only looking at the predictions, not the true labels; just equality between two ratios)\n",
      "\n",
      "What constitutes a discriminatory algorithm?\n",
      "How the law (in the US) conceives of discrimination (predating algorithms)\n",
      "Disparate treatment (procedural justice): The decision was made because of a protected attribute\n",
      "But-for causation: if the protected attribute was different, the decision would have been different. \n",
      "Disparate impact (distributive justice): Disproportionate effect on a protected class, even if unintentional\n",
      "Must be both unjustified and avoidable\n",
      "\n",
      "Avoiding disparate treatment does not always ensure we also prevent disparate impact, and vice versa. \n",
      "\n",
      "Algorithmic fairness deals with algorithmic outputs that may result in disparate outcomes or other forms of injustices for subgroups of the population, especially those who have been historically marginalized.\n",
      "\n",
      "How do we know if there is bias? To measure bias, we need:\n",
      "A definition of bias\n",
      "A method to measure it\n",
      "Four types of mathematical approaches to quantitatively measure bias:\n",
      "Statistical/group measures\n",
      "Most popular given the practical feasibility of implementation.\n",
      "Quantify statistical disparities across groups.\n",
      "Measures based on confusion matrix.\n",
      "Multiple possible measures, which disparity do we care about? Depends on the business context.\n",
      "Different measures correspond to different philosophical.political notions of fairness, and their relevance may also vary across contexts.\n",
      "Not posible to simultaneously satisfy all the different measures\n",
      "Similarity measures\n",
      "Causal measures\n",
      "Utility-based measures\n",
      "\n",
      "In training an ML model, assume g represents a sensitive attribute (e.g gender), and is an additional piece of info required that may or may not be part of input X. How can we compare the true labels, y, and the predicted values, y hat, and are the predictions biased when considering g?\n",
      "\n",
      "Confusion Matrix\n",
      "TP: Predicted is True, Actual is True\n",
      "FP: Predicted is True, Actual is False\n",
      "TN: Predicted is False, Actual is False\n",
      "FN: Predicted is False, Actual is True\n",
      "\n",
      "For all measures below, we compare between two different groups, and see if the values between the three are equal or what the difference is between them.\n",
      "\n",
      "Demographic Parity:\n",
      "Is the rate of people of different groups predicted to have a certain outcome the same? In other words, it is: # predicted / total population.\n",
      "Somewhat grounded on legal notion of the 80% rule: if selection rates for a job are sufficiently large across groups, it is considered a presumption of adverse impact.\n",
      "Does not consider the observed/true outcome\n",
      "Advantage: Sometimes observed outcome is biased\n",
      "Disadvantage: Sometimes the true rates are different, so a perfect prediction give different rates for different groups. \n",
      "Equalized Odds (Recall): \n",
      "TP /TP + FN\n",
      "Are the predictive errors biased against a certain group?\n",
      "Considers what the observed outcome is. \n",
      "Out of all of the true “positive” class, how many did we correctly predict?\n",
      "Predictive Parity (Precision):\n",
      "TP/TP + FP\n",
      "Are the precision rates biased against a certain group?\n",
      "Probability that a prediction is correct does not depend on the sensitive attribute. \n",
      "Out of all the predicted “positive” class, how many did we correctly predict?\n",
      "\n",
      "Examples:\n",
      "\n",
      "\n",
      "\n",
      "DP: 4/8 = 4/8\n",
      "EQ-Odds: 1/1 = 3/3 (or from FP perspective, 3/7 ≠ 1/5)\n",
      "Pred-Par: 1/4 ≠ 3/4\n",
      "\n",
      "\n",
      "\n",
      "DP: 2/6 = 4/12\n",
      "EQ-Odds: 1/2 = 2/4 (or from FP perspective, 1/4 = 2/8)\n",
      "Pred-Par: 1/2 = 2/4\n",
      "\n",
      "\n",
      "DP: 2/6 ≠ 4/8\n",
      "EQ-Odds: 1/3 ≠ 2/3 (or from FP perspective, 1/3 ≠ 2/5)\n",
      "Pred-Par: 1/2 = 2/4\n",
      "\n",
      "We cannot satisfy everything at once!\n",
      "If the base rates (rate of positives) are different, i.e. P(Y=1 | g = 1) ≠ P(Y=1 | g = 0), then we cannot simultaneously satisfy the three different fairness metrics we have studied. (In other words, if DP is not met, at least one of EQ-Odds or Pred-Parity will also not be met)\n",
      "If the base rates (rate of positives) are the same, algorithmic fairness metrics can be simultaneously satisfied. (In other words, if DP met, it is possible for both EQ-Odds and Pred-Par to be met simultaneously)\n",
      "How do we choose which metric(s) are relevant?\n",
      "It may be useful to report on multiple metrics, and reason over what these mean for a given case.\n",
      "When we are going to enforce a certain metric, or use as criteria to choose among multiple algorithms, we may need to choose which metric(s) we care about.\n",
      "Guiding criteria:\n",
      "What are the goods and burdens allocated by the possible decisions?\n",
      "What are the previous harms and injustices that the algorithm risks compounding?\n",
      "How will the choice of metric affect other desiderata?\n",
      "Different metrics may be more relevant to some stakeholders than others\n",
      "\n",
      "Post Exam 1\n",
      "\n",
      "Inclusion of Sensitive Attributes\n",
      "\n",
      "Grounded on the notion of disparate treatment, we may be tempted to think that omitting\n",
      "sensitive attributes will prevent algorithmic bias.\n",
      "\n",
      "However, omitting sensitive attributes is neither necessary nor sufficient.\n",
      "In some cases, including sensitive attributes may reduce group fairness gaps\n",
      "In other cases, removing sensitive attributes may only partially reduce group fairness gaps\n",
      "\n",
      "Compounding Injustice: If initial imbalance constitutes injustice: Model’s prediction is informed by, and compounds, previous injustice.\n",
      "Essentially, what this is saying is that if the starting point already has unfair differences among people, it's considered wrong or unjust. From there, if a model (like an algorithm or software) makes predictions based on this initial unjust situation, it will make the unfairness worse. Essentially, the model uses the existing unfair data and then acts in a way that adds to or intensifies that injustice, rather than correcting it.\n",
      "\n",
      "\n",
      "Different Types of Approaches to Mitigate Bias\n",
      "\n",
      "\tPre-Processing Bias Mitigation (Data): Modifications to data prior to training algorithm\n",
      "\n",
      "\t\tResampling\n",
      "\n",
      "Oversample statistical minority so that there are same number of instances from each group. \n",
      "\n",
      "Undersample statistical majority so that there are same number of instances from each group. \n",
      "\n",
      "There are different “flavors” of resampling - such as a mix of over and undersampling. \n",
      "\n",
      "\t\tReweighting\n",
      "Many ML algorithms allow you to provide weights for each instance, which are used during the training process.\n",
      "\n",
      "The default is that each instance has a weight of 1.\n",
      "If some instances are less/more important, we can reweight.\n",
      "Commonly used approach when one type of error is more costly than others, or where there is class imbalance.\n",
      "\n",
      "When using reweighting for group fairness, the weight corresponds to the group imbalance\n",
      "\n",
      "Weights can be updated so that sum of weights is the same across groups\n",
      "\n",
      "\n",
      "\n",
      "Formula Breakdown:\n",
      "\n",
      "Assume each instance (e.g., a person or a data point) in the dataset has a weight, which we can denote as Wi\n",
      "When you add up the weights of all instances in one group (Group 1), it equals the sum of the weights of all instances in another group (Group 2).\n",
      "The formula is simply saying that the total weight for Group 1 is the same as for Group 2 after reweighting.\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "To solve this question, we have the % of women in the dataset, so we can get the number of men in the dataset as 100% - 14.6%. This results in 85.4%. To have equal total weight, a woman's instance weight multiplied by 14.6% should equal a man's instance weight multiplied by 85.4%. So, we can do the % of men in the dataset divided by the % of women in the dataset. So, 85.4/14.6. This results in 5.8.\n",
      "\n",
      "\tTraining-Time Constraints (Training): Modifications to the machine learning algorithm \n",
      "\n",
      "\t\tDecoupled Classifiers\t\t\n",
      "\n",
      "\t\tEach decoupled classifier is trained with a portion of data that belongs to that subgroup. \n",
      "So from an X dataset, you would split/filter X into different subsets based on the different classes the sensitive variable, g, can take. \n",
      "Each subset of X would get its own unique function applied to it to come up with predictions. \n",
      "\n",
      "While there are different considerations/approaches for merging predictions, typically, we use the output probabilities of each of the unique functions per subset of X without modification. \n",
      "\n",
      "This is motivated by differential subgroup validity.\n",
      "\n",
      "Training a separate classifier allows us to capture group-specific patterns. \n",
      "\n",
      "\t\tRegularized Loss\n",
      "\n",
      "Typically, when thinking about how we apply a function to take input data X to predict y, we are solving some sort of optimization problem. \n",
      "For instance, we could be trying to maximize some sort of loss function based on parameters we are learning. This loss function depends on what algorithm we are training. \n",
      "\t\t\n",
      "\t\tWe can add a penalty to the function we are trying to optimize, such as a gap in TPR.\n",
      "\n",
      "\tPost-Processing (Output): Modifications to the output of a machine learning algorithm\n",
      "\n",
      "\t\tGroup specific classification thresholds \n",
      "\n",
      "Many ML algorithms provide an output that is a score between 0 and 1. If calibrated, this corresponds to a probability\n",
      "\n",
      "If scores are calibrated and we want to maximize accuracy, threshold 𝝉 = 0.5\n",
      "Other goals can lead to other thresholds, e.g. low FPR tolerance\n",
      "\n",
      "We can choose group specific classification thresholds such that a specific fairness measure (such as the equalized FPR) is satisfied.\n",
      "\n",
      "This pertains to what probability threshold is set for each group such that if they are above that threshold, they will be classified as the positive class, otherwise they are the negative class. We essentially make this different across groups. \t\n",
      "\n",
      "Training time constraints, such as regularized loss, are not appropriate when source of bias is a label bias or there is no data available for one subgroup. It is appropriate when there is differential subgroup validity. \n",
      "This is because if you have no data for a group, regularized loss won't help; you can't adjust for bias in a group you have no information about. You need data to understand and adjust for biases using techniques like regularized loss. If there's no data for a subgroup, the model can't learn about it, regardless of the training constraints you apply. Additionally, regularized loss helps address overfitting but doesn't directly correct biases in the labels themselves. If the problem is that the data you're using to train the model is biased, simply regularizing won't fix the issue; you need to correct the labels or get better data.\n",
      "\n",
      "In settings in which disparate treatment is undesirable, decoupled classifiers are not appropriate and\n",
      "therefore it is desirable to use post-processing approaches, such as group-specific thresholds. This is false. \n",
      "This is because if disparate treatment is undesirable, you wouldn't want to use different thresholds for different groups, as that would be a form of disparate treatment. Instead, you'd want to apply the same standards to everyone, adjusting the model as a whole to be fairer, rather than adjusting the outcomes for different groups separately.\n",
      "\n",
      "Utilitiy-Fairness Relationship\n",
      "\n",
      "If fairness is a constant, we don’t always pay a price; there is not always a tradeoff between utility and fairness.\n",
      "\n",
      "An optimization problem is not necessarily convex, so there may be more than one solution. Therefore, introducing a fairness constraint does not necessarily lead to reduced utility. \n",
      "Many solutions are equivalent in terms of utility but different in terms for fairness. \n",
      "\n",
      "When considering accuracy affecting injustices, many interventions may improve both\n",
      "utility and fairness.  \n",
      "More data\n",
      "Different data\n",
      "Different features\n",
      "Different target label\n",
      "\n",
      "When considering non-accuracy affecting injustices, there is an inherent trade-off.\n",
      "\t\t\n",
      "Appropriate Use of Mitigation Strategies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Basics of LLMs\n",
      "\n",
      "Natural language processing (NLP) poses machine learning tasks that are a little different\n",
      "from the supervised learning paradigm we have focused on. As a result, the machine learning\n",
      "models are also different.\n",
      "\n",
      "A core task in NLP is learning how to represent text in a structured format. We need to go\n",
      "from unstructured data to structured data.\n",
      "\n",
      "History of Text Representations:\n",
      "Bag of Words: Take each unique word as a feature/independent variable in order to represent the content holistically.\n",
      "Fancier versions (td-idf: term frequency/inverse document frequency): how important a term is within a document relative to a collection of documents\n",
      "Pre-Trained Embeddings\n",
      "Learned representations using lots of data\n",
      "Cosine similarity → meaning similarity\n",
      "Use as a representation for new tasks\n",
      "What do we do with words that have multiple meanings?\n",
      "Contextual Word Embeddings: Embedding is not fixed, but depends on the context in which the word appears.\n",
      "Ex: I crossed the river to get to the bank vs I crossed the street to get to the bank\n",
      "A core task is predicting the likely sequency of words: what is statistically plausible?\n",
      "Transformers: Core building block of LLMs: predicting what is plausible sequence of words based on enormous amounts of data,\n",
      "Enabled parreleizing the process: faster computation  \n",
      "More data led to better performance (no learning curve plateau)\n",
      "\n",
      "LLMs we know today have two core building blocks:\n",
      "Pretrained language model (enabled by transformers)\n",
      "RLHF: Reinforcement Learning from Human Feedback\n",
      "Human feedback used to improve the language model: Human annotators rank outputs of LLMs, then that feedback is used to improve the LLM. \n",
      "\n",
      "The general flow of an LLM is from a prompts dataset, we sample many prompts which go into an initial LLM. From there, the output of the initial LLM is scored and ranked by a human. This initial output and the corresponding score/rank is then used to train a reward (preference) model, which is used to update the overall language model, subject to a constraint of not changing the model too much. \n",
      "\n",
      "LLMs predict the most likely sequence of text (based on what is statistically likely in a huge corpus) that will be rewarded by human annotators. \n",
      "\n",
      "Issues to keep in mind:\n",
      "Data rights: Whose data is used to train the system?\n",
      "What are the environmental costs of training the system?\n",
      "Is what is statistically frequent what we always want?\n",
      "Who are the humans doing the scoring and shaping the system? Whose views are encoded?\n",
      "What are the conditions of workers employed to do this work?\n",
      "\n",
      "When an LLM outputs a text, we can be certain the same text appears at least once in the training data. This is false. This is because LLMs are not simple lookup tables that output text directly from the training data. Instead, they are trained to understand patterns and relationships between words, phrases, and sentences so that they can generate text that is similar in structure and meaning to the training data but not necessarily an exact copy. The training process involves adjusting internal parameters (weights) based on the vast amount of text the model is exposed to. This training enables the model to predict the next word in a sequence in a way that is statistically likely, given the context provided by the words that come before it. \n",
      "\n",
      "\n",
      "Processing PDF: Exam Review Sheet_ Ethics of AI.pdf\n",
      "Exam 1 Artifacts and Abstraction (Sessions 0 - 2) There are two ways in which artifacts can contain political properties, where we define political as an arrangement of power and authority in human association as well as the activities that take place within those arrangements: 1. Technical arrangements as a form of order a. Precedes use of the actual thing b. Intended c. Can also transcend simple category of “intended” and “unintended” d. Example: Low hanging overpasses on Long Island designed to enforce social segregation; blocks off beach access to low-income people e. Example: Park benches and homeless; no handrests makes it harder for them to lie down on it. 2. Inherently political technologies a. Adoption of a certain technology requires or is strongly compatible with certain social conditions. b. Example: Nuclear power plants encourage centralized, hierarchical control structures due to the complexity and potential hazards associated with nuclear technology. This need for centralized control and specialized expertise leads to a form of social organization that is highly structured and authoritarian, reflecting and reinforcing certain power dynamics within society. c. Example: Cotton mills; people had to work a very specific way and in a certain timeframe; sequence is super important (do exactly what you are told to do) False Example: Energy sources have politics because the impact they have on climate change is an inherently political property. Main AI Ingredient = Data We define AI as the “use of technologies to build machines and computers that have the ability to mimic cognitive functions associated with human intelligence.” We define ML as a subset of AI, and it is the “use of algorithms to automatically learn insights and recognize patterns from data.” We define supervised learning as a subset of ML, and it is the “use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.” Values Embedded in ML Pipeline (Sessions 3 - 6) Basics of supervised learning: We take input data, X, and using a function f(X), predict known labels,Y.\n",
      "Machine Learning Pipeline: 1. Problem formulation a. What is the overarching goal of the system? i. Different stakeholders can have different goals. b. What is the mechanism of entry into the population subjected to the algorithm? i. Need to consider who will encounter the algorithm and if they have a choice. ii. Not determined by the sampling approach used to collect training data! It is who will encounter the algorithm! c. What is the space of possible decisions? i. Need to consider the decisions the algorithm is informing and alternative decision spaces. This refers to the different decisions that could be made with the algorithm. d. Algorithms that solve seemingly the same task can be embedded within entirely different problem formulations, which directly impacts fairness considerations. i. Need to consider what burdens and benefits are being allocated. e. Some tasks may be inherently biased and grounded on discriminatory assumptions. f. It can be misleading to talk about “fairness”; can be narrowly defined as disparities in performance. g. Accuracy and good AI performance can also be harmful! 2. Data collection and representation a. Garbage in = Garbage out. b. ML algorithms are often trained with convenient, inexpensive data. c. Big data ≠ Good data. d. Is the data collection ethical? i. Need to consider if the people that are represented have given consent, and if that consent is meaningful. 1. People can be mislead into giving consent or have no alternative. ii. The data market may be noxious in that our personal data may be commoditized (along with our privacy right), which threatens equal fundamental political rights and freedoms, and equal rights to a fair share of social welfare and equality of opportunity; morally objectionable; harmful outcomes to individuals or society. e. Sampling Bias: Who is represented? i. Occurs when the distribution of the sampled data does not match the distribution of the population that will encounter the algorithm ii. This matters because the resulting bias can have severe consequences, such as in responding to disasters (among other tasks). iii. Sources: 1. Access to technology and resources. 2. Previously served/underserved communities. 3. Trust in authorities (reporting). iv. Invisibilization in sampling bias often compounds previous injustices. f. Differential Subgroup Validity: How are people represented? i. Predictive power of features collected may differ across subpopulations.\n",
      "1. Example: In healthcare, symptoms that are studied, taught, and recorded may only hold diagnostic power for some (skintones). ii. Choice may be informed by, and only hold predictive power, in some cultural contexts. 1. Number of credit cards as a positive signal for “creditworthiness.” iii. Ability to adapt to a certain choice of features may also differ across groups. Moreover, strategic adaptation to incentives is not possible for everyone. 1. Example: standardized tests - we can incentivize students to invest in tutoring and retake tests, but not everyone can do this. iv. Can we measure what we care about: mind the gap 1. Y → Quantifiable, imperfect proxy (substitute for ideal target variable) 2. Yc → Complex, multi-faceted (and sometimes contested) outcomes; what we aim for, but even this can be debated (think of capstone) 3. Misleading comparisons: a. Human vs Machine is not necessarily a fair comparison ; calling one of them “better” can be misleading because it depends on the context; a human and machine may not engage in the same predictive task; it depends on context. 4. Self-fulfilling prophecy: When you assume the proxy is a good target variable substitute, this effect overall outcome prediction evaluations v. Human assessments encoded as labels: 1. Target labels are often termed “ground truth”, but they may also encode biases. We have to consider whose views are encoded and valued when humans determine the target labels. g. Example Question: An organization is developing a disaster relief system that can support rescue efforts during natural disasters. The algorithm identifies anomalous patters in social media activity. However, a team inside the organization is concerned that this tool may not provide equal support to all communities. The salient type of bias that this is likely to affect this system is: sampling bias. 3. Optimization objectives and evaluation (can go back to 2) a. How do we define good? What does “good performance” mean for an AI system? What is the optimization objective? How do we evaluate performance? We need to consider all of these question. b. Optimizing for overall performance is not ideal because it does not implicitly optimize performance for all subgroups. It can actually lead to disparate performance across subgroups. c. Common predictive performance metric: ROC AUC (Area under the ROC Curve). d. There is no one-size-fits-all metric of success: i. In healthcare, we often care about performance at low false negative rates (diabetes example) as opposed to fasle positive rates. This differs for other business contexts. e. In many cases accuracy does not capture our business needs. i. Example: A third party platform like Zillow, who could be making an algorithm to price houses, has to maintain both sides between buyer and the seller.\n",
      "Balancing between overestimating and underestimating in predictions can be tricky, and accuracy does not capture this balance. f. Accuracy can be misleading. i. Example: For a system with 90% accuracy, it can be useless if you classify everything as one class and this accuracy is a result of the data distribution. g. In ML we often take for granted that “overall performance” is the desired goal. Is something that works “for most” enough? We do not think so for many instances in our society. i. Example: Stairs work for 90% of people, but what about the 10% in wheelchairs? 4. Algorithmic adoption (can go back to 2 or 1) a. Incentives for deployment (and overarching goals); need to consider the following i. How is a certain technology integrated into a broader social technical system? 1. In the case of AI-assisted decisions, how are algorithmic recommendations integrated into decisions? ii. Ethical implications for future of work ; need to consider the following: 1. Who gains from this efficiency? Who owns the labor? 2. Different possibilities under different social/power organizations? 3. Are there trade-offs? 4. Efficiency at what cost? iii. How do algorithmic recommendations interact with human beliefs, values, and abilities?; need to consider the following 1. AI technologies may exacerbate ethical concerns in a socialtechnical system (and humans may exacerbate ethical concerns of algorithm) iv. Feedback loops and algorithmic manipulation: 1. Once deployed, algorithms are actors that alter the state of the world 2. There are two key ways in which this affects the ethical properties of the algorithm: a. Algorithms can manipulate us: is an algorithm that predicts what you are likely to buy addressing your needs/wants or is it altering what you think you want? 3. Once deployed, algorithms create incentives ; people and organizations may adapt to it a. Different people may have different resources/abilities to adapt (recall standardized tests) 4. Goodhart’s Law: When a measure becomes a target, it ceases to be a good measure. a. In other words, when we use a measure to reward performance, we provide an incentive to manipulate the measure in order to receive the reward. Algorithmic Bias Detection (Sessions 7 - 9) Algorithmic Bias: - One concrete type of harm that may stem from the use of ML\n",
      "- Defined as algorithmic outputs that may result in disparate outcomes or other forms of injustices for subgroups of the population, especially those who have been historically marginalized - Important on the basis of legal compliance, social responsibility, and utility Why is discrimination wrong? - Argument can be made that the whole point of using ML algorithms to discriminate - Systematic relative disadvantage: treatment that systematically imposes a disadvantage on one social group relative to others. - Different normative underpinning for why it is wrong: - Relevance: Relying on characteristics that bear little or no relevance to the outcome or quality that decision makers might be trying to predict or assess - Generalizations: Needlessly coarse groupings - Is there additional info that could provide a more granular view? - Example: Provide a fitness test to all firefighter applicants instead of banning women. - Prejudice: When decision makers hold entire groups in lesser regard than others - Disrespect: Casting certain groups as categorically inferior to others and thus not worthy of equal respect - Different from prejudice because it is about the message that the actions of the decision makers is sending. - Immutability: Treating people differently according to characteristics over which they possess no control. - Compounding injustice: People cannot be morally culpable for certain facts about themselves that are not the result of their own actions. - This is especially the case if these facts are the result of some past injustice. Can it be unjust if it is accurate? - It may depend on your normative grounding. - Grounded on a compounding injustice view, there may be non-accuracy affecting injustices: - Accuracy-affecting injustices: Either the data or output of a model inaccurately estimate a fact about people. (Accuracy is inaccurate) - EX: Gaps in FP or FN Rates - emphasis on whether prediction is correct or rate we are incurring errors - Non-accuracy-affecting injustices: Data and models accurately estimate a fact about people, but these traits themselves result from injustice. - EX: Demographic Parity (gives us different grounds for why we want a certain rate to be the same; only looking at the predictions , not the true labels; just equality between two ratios) What constitutes a discriminatory algorithm? - How the law (in the US) conceives of discrimination (predating algorithms) - Disparate treatment (procedural justice): The decision was made because of a protected attribute\n",
      "- But-for causation: if the protected attribute was different, the decision would have been different. - Disparate impact (distributive justice): Disproportionate effect on a protected class, even if unintentional - Must be both unjustified and avoidable Avoiding disparate treatment does not always ensure we also prevent disparate impact, and vice versa. Algorithmic fairness deals with algorithmic outputs that may result in disparate outcomes or other forms of injustices for subgroups of the population, especially those who have been historically marginalized. How do we know if there is bias? To measure bias, we need: - A definition of bias - A method to measure it - Four types of mathematical approaches to quantitatively measure bias: - Statistical/group measures - Most popular given the practical feasibility of implementation. - Quantify statistical disparities across groups. - Measures based on confusion matrix. - Multiple possible measures, which disparity do we care about? Depends on the business context. - Different measures correspond to different philosophical.political notions of fairness, and their relevance may also vary across contexts. - Not posible to simultaneously satisfy all the different measures - Similarity measures - Causal measures - Utility-based measures In training an ML model, assume g represents a sensitive attribute (e.g gender), and is an additional piece of info required that may or may not be part of input X. How can we compare the true labels, y, and the predicted values, y hat, and are the predictions biased when considering g? 1. Confusion Matrix a. TP: Predicted is True, Actual is True b. FP: Predicted is True, Actual is False c. TN: Predicted is False, Actual is False d. FN: Predicted is False, Actual is True For all measures below, we compare between two different groups, and see if the values between the three are equal or what the difference is between them. 2. Demographic Parity: a. Is the rate of people of different groups predicted to have a certain outcome the same? In other words, it is: # predicted / total population.\n",
      "b. Somewhat grounded on legal notion of the 80% rule: if selection rates for a job are sufficiently large across groups, it is considered a presumption of adverse impact. c. Does not consider the observed/true outcome i. Advantage: Sometimes observed outcome is biased ii. Disadvantage: Sometimes the true rates are different, so a perfect prediction give different rates for different groups. 3. Equalized Odds (Recall): a. TP /TP + FN b. Are the predictive errors biased against a certain group? c. Considers what the observed outcome is. d. Out of all of the true “positive” class, how many did we correctly predict? 4. Predictive Parity (Precision): a. TP/TP + FP b. Are the precision rates biased against a certain group? c. Probability that a prediction is correct does not depend on the sensitive attribute. d. Out of all the predicted “positive” class, how many did we correctly predict? Examples: DP: 4/8 = 4/8 EQ-Odds: 1/1 = 3/3 (or from FP perspective, 3/7 ≠ 1/5) Pred-Par: 1/4 ≠ 3/4\n",
      "DP: 2/6 = 4/12 EQ-Odds: 1/2 = 2/4 (or from FP perspective, 1/4 = 2/8) Pred-Par: 1/2 = 2/4 DP: 2/6 ≠ 4/8 EQ-Odds: 1/3 ≠ 2/3 (or from FP perspective, 1/3 ≠ 2/5) Pred-Par: 1/2 = 2/4 We cannot satisfy everything at once! - If the base rates (rate of positives) are different, i.e. P(Y=1 | g = 1) ≠ P(Y=1 | g = 0), then we cannot simultaneously satisfy the three different fairness metrics we have studied. (In other words, if DP is not met, at least one of EQ-Odds or Pred-Parity will also not be met) - If the base rates (rate of positives) are the same, algorithmic fairness metrics can be simultaneously satisfied. (In other words, i f DP met, it is possible for both EQ-Odds and Pred-Par to be met simultaneously) - How do we choose which metric(s) are relevant? - It may be useful to report on multiple metrics, and reason over what these mean for a given case.\n",
      "- When we are going to enforce a certain metric, or use as criteria to choose among multiple algorithms, we may need to choose which metric(s) we care about. - Guiding criteria: - What are the goods and burdens allocated by the possible decisions? - What are the previous harms and injustices that the algorithm risks compounding? - How will the choice of metric affect other desiderata? - Different metrics may be more relevant to some stakeholders than others Post Exam 1 Inclusion of Sensitive Attributes Grounded on the notion of disparate treatment, we may be tempted to think that omitting sensitive attributes will prevent algorithmic bias. However, omitting sensitive attributes is neither necessary nor sufficient. - In some cases, including sensitive attributes may reduce group fairness gaps - In other cases, removing sensitive attributes may only partially reduce group fairness gaps Compounding Injustice: If initial imbalance constitutes injustice: Model’s prediction is informed by, and compounds, previous injustice. - Essentially, what this is saying is that if the starting point already has unfair differences among people, it's considered wrong or unjust. From there, if a model (like an algorithm or software) makes predictions based on this initial unjust situation, it will make the unfairness worse. Essentially, the model uses the existing unfair data and then acts in a way that adds to or intensifies that injustice, rather than correcting it. Different Types of Approaches to Mitigate Bias Pre-Processing Bias Mitigation (Data) : Modifications to data prior to training algorithm Resampling Oversample statistical minority so that there are same number of instances from each group. Undersample statistical majority so that there are same number of instances from each group. There are different “flavors” of resampling - such as a mix of over and undersampling. Reweighting\n",
      "Many ML algorithms allow you to provide weights for each instance, which are used during the training process. - The default is that each instance has a weight of 1. - If some instances are less/more important, we can reweight. - Commonly used approach when one type of error is more costly than others, or where there is class imbalance. When using reweighting for group fairness, the weight corresponds to the group imbalance Weights can be updated so that sum of weights is the same across groups Formula Breakdown: - Assume each instance (e.g., a person or a data point) in the dataset has a weight, which we can denote as Wi - When you add up the weights of all instances in one group (Group 1), it equals the sum of the weights of all instances in another group (Group 2). - The formula is simply saying that the total weight for Group 1 is the same as for Group 2 after reweighting. To solve this question, we have the % of women in the dataset, so we can get the number of men in the dataset as 100% - 14.6%. This results in 85.4%. To have\n",
      "equal total weight, a woman's instance weight multiplied by 14.6% should equal a man's instance weight multiplied by 85.4%. So, we can do the % of men in the dataset divided by the % of women in the dataset. So, 85.4/14.6. This results in 5.8. Training-Time Constraints (Training) : Modifications to the machine learning algorithm Decoupled Classifiers Each decoupled classifier is trained with a portion of data that belongs to that subgroup. - So from an X dataset, you would split/filter X into different subsets based on the different classes the sensitive variable, g, can take. - Each subset of X would get its own unique function applied to it to come up with predictions. While there are different considerations/approaches for merging predictions, typically, we use the output probabilities of each of the unique functions per subset of X without modification. This is motivated by differential subgroup validity. Training a separate classifier allows us to capture group-specific patterns. Regularized Loss Typically, when thinking about how we apply a function to take input data X to predict y, we are solving some sort of optimization problem. - For instance, we could be trying to maximize some sort of loss function based on parameters we are learning. This loss function depends on what algorithm we are training. We can add a penalty to the function we are trying to optimize, such as a gap in TPR. Post-Processing (Output): Modifications to the output of a machine learning algorithm Group specific classification thresholds Many ML algorithms provide an output that is a score between 0 and 1. If calibrated, this corresponds to a probability If scores are calibrated and we want to maximize accuracy, threshold 𝝉 = 0.5 - Other goals can lead to other thresholds, e.g. low FPR tolerance\n",
      "We can choose group specific classification thresholds such that a specific fairness measure (such as the equalized FPR) is satisfied. This pertains to what probability threshold is set for each group such that if they are above that threshold, they will be classified as the positive class, otherwise they are the negative class. We essentially make this different across groups. Training time constraints, such as regularized loss, are not appropriate when source of bias is a label bias or there is no data available for one subgroup. It is appropriate when there is differential subgroup validity. - This is because if you have no data for a group, regularized loss won't help; you can't adjust for bias in a group you have no information about. You need data to understand and adjust for biases using techniques like regularized loss. If there's no data for a subgroup, the model can't learn about it, regardless of the training constraints you apply. Additionally, regularized loss helps address overfitting but doesn't directly correct biases in the labels themselves. If the problem is that the data you're using to train the model is biased, simply regularizing won't fix the issue; you need to correct the labels or get better data. In settings in which disparate treatment is undesirable, decoupled classifiers are not appropriate and therefore it is desirable to use post-processing approaches, such as group-specific thresholds. This is false. - This is because if disparate treatment is undesirable, you wouldn't want to use different thresholds for different groups, as that would be a form of disparate treatment. Instead, you'd want to apply the same standards to everyone, adjusting the model as a whole to be fairer, rather than adjusting the outcomes for different groups separately. Utilitiy-Fairness Relationship If fairness is a constant, we don’t always pay a price; there is not always a tradeoff between utility and fairness. An optimization problem is not necessarily convex, so there may be more than one solution. Therefore, introducing a fairness constraint does not necessarily lead to reduced utility. - Many solutions are equivalent in terms of utility but different in terms for fairness. When considering accuracy affecting injustices, many interventions may improve both utility and fairness. - More data - Different data - Different features - Different target label When considering non-accuracy affecting injustices, there is an inherent trade-off. Appropriate Use of Mitigation Strategies\n",
      "Basics of LLMs Natural language processing (NLP) poses machine learning tasks that are a little different from the supervised learning paradigm we have focused on. As a result, the machine learning models are also different. A core task in NLP is learning how to represent text in a structured format. We need to go from unstructured data to structured data. History of Text Representations: 1. Bag of Words: Take each unique word as a feature/independent variable in order to represent the content holistically. a. Fancier versions (td-idf: term frequency/inverse document frequency): how important a term is within a document relative to a collection of documents 2. Pre-Trained Embeddings a. Learned representations using lots of data b. Cosine similarity → meaning similarity\n",
      "c. Use as a representation for new tasks d. What do we do with words that have multiple meanings? i. Contextual Word Embeddings: Embedding is not fixed, but depends on the context in which the word appears. 1. Ex: I crossed the river to get to the bank vs I crossed the street to get to the bank e. A core task is predicting the likely sequency of words: what is statistically plausible? f. Transformers: Core building block of LLMs: predicting what is plausible sequence of words based on enormous amounts of data, i. Enabled parreleizing the process: faster computation ii. More data led to better performance (no learning curve plateau) LLMs we know today have two core building blocks: 1. Pretrained language model (enabled by transformers) 2. RLHF: Reinforcement Learning from Human Feedback a. Human feedback used to improve the language model: Human annotators rank outputs of LLMs, then that feedback is used to improve the LLM. The general flow of an LLM is from a prompts dataset, we sample many prompts which go into an initial LLM. From there, the output of the initial LLM is scored and ranked by a human. This initial output and the corresponding score/rank is then used to train a reward (preference) model, which is used to update the overall language model, subject to a constraint of not changing the model too much. LLMs predict the most likely sequence of text (based on what is statistically likely in a huge corpus) that will be rewarded by human annotators. Issues to keep in mind: 1. Data rights: Whose data is used to train the system? 2. What are the environmental costs of training the system? 3. Is what is statistically frequent what we always want? 4. Who are the humans doing the scoring and shaping the system? Whose views are encoded? 5. What are the conditions of workers employed to do this work? When an LLM outputs a text, we can be certain the same text appears at least once in the training data. This is false. This is because LLMs are not simple lookup tables that output text directly from the training data. Instead, they are trained to understand patterns and relationships between words, phrases, and sentences so that they can generate text that is similar in structure and meaning to the training data but not necessarily an exact copy. The training process involves adjusting internal parameters (weights) based on the vast amount of text the model is exposed to. This training enables the model to predict the next word in a sequence in a way that is statistically likely, given the context provided by the words that come before it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder = \"Documents\"  \n",
    "    process_documents(folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
