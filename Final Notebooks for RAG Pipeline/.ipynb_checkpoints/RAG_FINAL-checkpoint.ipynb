{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cb6731-d654-4fad-9446-d358c0e511f2",
   "metadata": {},
   "source": [
    "## The following RAG notebook covers the following: \n",
    "### 1. Vectorizing all the content being fetched (Data Ingestion)\n",
    "### 2. Storing all the embeddings into a vector database and retrieving it (Retrieval)\n",
    "### 3. Querying the LLM (Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f05a66-a557-4f47-b976-6d6d566b0a31",
   "metadata": {},
   "source": [
    "## Part 1: Vectorization of all the content being fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1c82f-5bbf-4a0a-9616-b2e6b0fbc9ce",
   "metadata": {},
   "source": [
    "### Loading and preparing the JSON Data (Data Ingestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8de0a-067d-467f-9d80-3bfb40f0bc4b",
   "metadata": {},
   "source": [
    "Install the below dependencies first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1145356-445a-4b04-b531-4feba0115bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0647dbe-01ad-4e01-861d-e3b352c3c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8fdc9e3-7937-4b3f-84da-4d486b1a825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits the input text into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to split.\n",
    "        chunk_size: Maximum number of words per chunk.\n",
    "        overlap: Number of overlapping words between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_documents(folder_path, chunk_size=500, overlap=50):\n",
    "    \"\"\"Process all PDF and DOCX files with chunking for large documents.\"\"\"\n",
    "    extracted_items = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            print(f\"Processing PDF: {filename}\")\n",
    "            pdf_text = extract_text_from_pdf(file_path)\n",
    "            if pdf_text.strip():\n",
    "                # Split into chunks if the text is long\n",
    "                words = pdf_text.split()\n",
    "                if len(words) > chunk_size:\n",
    "                    chunks = chunk_text(pdf_text, chunk_size, overlap)\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        content_item = {\n",
    "                            \"video_id\": filename,\n",
    "                            \"frame\": f\"chunk_{i+1}\",\n",
    "                            \"timestamp\": f\"chunk_{i+1}\",\n",
    "                            \"timestamp_seconds\": i,\n",
    "                            \"content\": chunk\n",
    "                        }\n",
    "                        extracted_items.append(content_item)\n",
    "                else:\n",
    "                    \n",
    "                    content_item = {\n",
    "                        \"video_id\": filename,\n",
    "                        \"frame\": \"full_text\",\n",
    "                        \"timestamp\": \"0:00\",\n",
    "                        \"timestamp_seconds\": 0,\n",
    "                        \"content\": pdf_text\n",
    "                    }\n",
    "                    extracted_items.append(content_item)\n",
    "        \n",
    "        elif filename.lower().endswith('.docx'):\n",
    "            print(f\"Processing Word Document: {filename}\")\n",
    "            word_text = extract_text_from_word(file_path)\n",
    "            if word_text.strip():\n",
    "                # Split into chunks if the text is long\n",
    "                words = word_text.split()\n",
    "                if len(words) > chunk_size:\n",
    "                    chunks = chunk_text(word_text, chunk_size, overlap)\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        content_item = {\n",
    "                            \"video_id\": filename,\n",
    "                            \"frame\": f\"chunk_{i+1}\",\n",
    "                            \"timestamp\": f\"chunk_{i+1}\",\n",
    "                            \"timestamp_seconds\": i,\n",
    "                            \"content\": chunk\n",
    "                        }\n",
    "                        extracted_items.append(content_item)\n",
    "                else:\n",
    "                    \n",
    "                    content_item = {\n",
    "                        \"video_id\": filename,\n",
    "                        \"frame\": \"full_text\",\n",
    "                        \"timestamp\": \"0:00\",\n",
    "                        \"timestamp_seconds\": 0,\n",
    "                        \"content\": word_text\n",
    "                    }\n",
    "                    extracted_items.append(content_item)\n",
    "    \n",
    "    return extracted_items\n",
    "\n",
    "\n",
    "\n",
    "def load_json_files(json_folder: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all JSON files from a folder and extract their content.\n",
    "    Handles both YouTube processed JSON files and document extraction JSON files.\n",
    "    \n",
    "    Args:\n",
    "        json_folder: Path to folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with extracted content.\n",
    "    \"\"\"\n",
    "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "    all_content = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        file_basename = os.path.basename(json_file)\n",
    "        print(f\"Processing: {file_basename}\")\n",
    "        \n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Determine type of JSON file based on structure\n",
    "            if isinstance(data, list):\n",
    "                # Check if this might be our PDF/DOCX extraction format\n",
    "                if len(data) > 0 and all(isinstance(item, dict) and \"video_id\" in item and \"content\" in item for item in data):\n",
    "                    print(f\"  Detected document extraction JSON format\")\n",
    "                    all_content.extend(data)\n",
    "                    print(f\"  Added {len(data)} content items from {file_basename}\")\n",
    "                else:\n",
    "                    # Process as YouTube formatted JSON\n",
    "                    file_content = []\n",
    "                    for i, frame in enumerate(data):\n",
    "                        frame_name = frame.get(\"frame\", f\"frame_{i}\")\n",
    "                        text_content = []\n",
    "                        \n",
    "                        if frame.get(\"caption\"):\n",
    "                            text_content.append(f\"Caption: {frame['caption']}\")\n",
    "                        if frame.get(\"extracted_text\"):\n",
    "                            text_content.append(f\"Text: {frame['extracted_text']}\")\n",
    "                        if frame.get(\"label\"):\n",
    "                            text_content.append(f\"Visual: {frame['label']}\")\n",
    "                        \n",
    "                        if text_content:\n",
    "                            frame_time = 0\n",
    "                            try:\n",
    "                                time_str = frame_name.replace(\"frame_\", \"\").replace(\".jpg\", \"\")\n",
    "                                frame_time = int(time_str)\n",
    "                            except:\n",
    "                                frame_time = i * 5\n",
    "                            \n",
    "                            time_min = frame_time // 60\n",
    "                            time_sec = frame_time % 60\n",
    "                            time_str = f\"{time_min}:{time_sec:02d}\"\n",
    "                            \n",
    "                            content_item = {\n",
    "                                \"video_id\": file_basename.replace(\"_processed.json\", \"\"),\n",
    "                                \"frame\": frame_name,\n",
    "                                \"timestamp\": time_str,\n",
    "                                \"timestamp_seconds\": frame_time,\n",
    "                                \"content\": \" \".join(text_content)\n",
    "                            }\n",
    "                            \n",
    "                            file_content.append(content_item)\n",
    "                    \n",
    "                    all_content.extend(file_content)\n",
    "                    print(f\"  Added {len(file_content)} content items from {file_basename}\")\n",
    "            else:\n",
    "                print(f\"  Skipping {file_basename}: Unrecognized format\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_basename}: {e}\")\n",
    "    \n",
    "    print(f\"Total content items extracted: {len(all_content)}\")\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559638e-208a-4acd-981b-6f4ff2884c80",
   "metadata": {},
   "source": [
    "### Conversion of .json files into embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcdb993-b725-4bd5-b6c7-7adde75d4270",
   "metadata": {},
   "source": [
    "We are using the all-miniLM-l6-v2 model for vectorizing the json files into embeddings. \n",
    "\n",
    "The hugging face link: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90043983-46ad-4e03-b543-998aa631539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_content(content_items: List[Dict[str, Any]],\n",
    "                      model_name: str = \"all-MiniLM-L6-v2\",\n",
    "                      chunk_size: int = 500,\n",
    "                      overlap: int = 50) -> Tuple[np.ndarray, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Convert content items to vector embeddings. For long texts, perform chunking.\n",
    "    \n",
    "    Args:\n",
    "        content_items: List of content items with text.\n",
    "        model_name: Name of the SentenceTransformer model to use.\n",
    "        chunk_size: Maximum number of words in each chunk.\n",
    "        overlap: Number of overlapping words between chunks.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (embeddings array, updated content items with chunk info).\n",
    "    \"\"\"\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"Loaded embedding model: {model_name}\")\n",
    "    \n",
    "    chunked_items = []\n",
    "    for item in content_items:\n",
    "        text = item[\"content\"]\n",
    "        # Chuking\n",
    "        if len(text.split()) > chunk_size:\n",
    "            chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                new_item = item.copy()\n",
    "                new_item[\"content\"] = chunk\n",
    "                new_item[\"chunk_index\"] = i\n",
    "                chunked_items.append(new_item)\n",
    "        else:\n",
    "            chunked_items.append(item)\n",
    "    \n",
    "    texts = [item[\"content\"] for item in chunked_items]\n",
    "    print(f\"Generating embeddings for {len(texts)} chunks/items...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return embeddings, chunked_items\n",
    "\n",
    "def save_vectors(embeddings: np.ndarray, content_items: List[Dict[str, Any]], output_folder: str):\n",
    "    \"\"\"\n",
    "    Save the vector embeddings and content items.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings.\n",
    "        content_items: List of content items.\n",
    "        output_folder: Folder to save the files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    embeddings_path = os.path.join(output_folder, \"embeddings.npy\")\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    \n",
    "    content_path = os.path.join(output_folder, \"content_items.pkl\")\n",
    "    with open(content_path, 'wb') as f:\n",
    "        pickle.dump(content_items, f)\n",
    "    \n",
    "    print(f\"Saved embeddings to {embeddings_path}\")\n",
    "    print(f\"Saved content items to {content_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec91e5f-470c-46a5-a96d-0faa0e5618c9",
   "metadata": {},
   "source": [
    "### Main Function call for chunking, vectorizing data and storing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e8b6cb-5c81-4f1f-b0d8-ceb0afff9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Added 38 content items from Ilg3gGewQ5U_processed_filtered.json\n",
      "Processing: extracted_text.json\n",
      "  Detected document extraction JSON format\n",
      "  Added 1 content items from extracted_text.json\n",
      "Total content items extracted: 39\n",
      "Extracted 39 total content items from all JSON files\n",
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Generating embeddings for 42 chunks/items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (42, 384)\n",
      "Saved embeddings to /Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/vectorized_data/embeddings.npy\n",
      "Saved content items to /Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/vectorized_data/content_items.pkl\n",
      "Vectorization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration folders\n",
    "    json_folder = '/Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/Data to be considered'\n",
    "    output_folder = '/Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/vectorized_data' \n",
    "    \n",
    "    content_items = load_json_files(json_folder)\n",
    "    print(f\"Extracted {len(content_items)} total content items from all JSON files\")\n",
    "    \n",
    "    embeddings, content_items = vectorize_content(content_items)\n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    save_vectors(embeddings, content_items, output_folder)\n",
    "    \n",
    "    print(\"Vectorization complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b802451-a71f-4d08-a67f-220dea0a4dee",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b3cbb-aa6c-443d-90c9-52396532b5e1",
   "metadata": {},
   "source": [
    "## Part 2: Storing all the embeddings into a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b129f-3c7b-4fd2-a282-a303cf5709d7",
   "metadata": {},
   "source": [
    "We will use the FAISS for searching embeddings\n",
    "\n",
    "Link for FAISS : https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451d3ac7-75a0-4f37-a0c4-26be9b908b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef46112-1a16-4dad-9463-a50b2316e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings with shape: (42, 384)\n",
      "Loaded 42 content items.\n",
      "FAISS index has 42 vectors.\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import glob\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def load_vectorized_data(embeddings_path: str, content_path: str):\n",
    "    \"\"\"\n",
    "    Load the saved embeddings (NumPy array) and content items (pickle file).\n",
    "    \"\"\"\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    with open(content_path, 'rb') as f:\n",
    "        content_items = pickle.load(f)\n",
    "    return embeddings, content_items\n",
    "\n",
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Create a FAISS index using cosine similarity (normalize + inner product).\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.astype(\"float32\")\n",
    "    \n",
    "    # Normalizing vectors for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "    index.add(embeddings)\n",
    "    print(f\"FAISS index has {index.ntotal} vectors.\")\n",
    "    return index\n",
    "\n",
    "def search_index(query: str, model: SentenceTransformer, index: faiss.Index, content_items: list, k: int = 5):\n",
    "    \"\"\"\n",
    "    Convert the query to an embedding, search the FAISS index for the top-k nearest neighbors,\n",
    "    and return the corresponding content items with their scores.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(query, show_progress_bar=False)\n",
    "    query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "    \n",
    "    \n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append((score, content_items[idx]))\n",
    "    return results\n",
    "\n",
    "\n",
    "## Change below accordingly \n",
    "\n",
    "embeddings_path = \"vectorized_data/embeddings.npy\"\n",
    "content_path = \"vectorized_data/content_items.pkl\"\n",
    "\n",
    "# Load vectorized data\n",
    "embeddings, content_items = load_vectorized_data(embeddings_path, content_path)\n",
    "print(f\"Loaded embeddings with shape: {embeddings.shape}\")\n",
    "print(f\"Loaded {len(content_items)} content items.\")\n",
    "\n",
    "# Create the FAISS index\n",
    "index = create_faiss_index(embeddings)\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0dd46-f4f0-4b54-b998-97f737d0a8a4",
   "metadata": {},
   "source": [
    "### Testing of the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3fac556-1d0f-4cbc-8b7e-0a21aef85bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "  Distance: 0.2824\n",
      "  Video ID: AML Quiz 1 Study Guide.pdf\n",
      "  Frame: full_text\n",
      "  Timestamp: 0:00\n",
      "  Content: 3 Types of Analytics 1. Descriptive - find human interpretable patterns that describe the data a. Large scale summaries, local patterns - finding fraud 2. Predictive - use variables to predict unknown or future values of other variables a. Regression, classification, ranking/recommendation 3. Prescriptive (may need causal reasoning) Analysis is often retrospective (not prospective) data was not collected in a methodical way that is tailored for the analytical task ● Data may be collected in the past without a specific analytical task in mind, which can lead to challenges because the data was not gathered with a methodology to ensure relevance or completeness for the analytics to be done - more risk for biases that are not controlled No Free Lunch - there is no universally best model, so tradeoffs need to be understood ● Deep nets and complex models are very general and powerful (few to no assumptions about the nature of relationships) but require lots of data, hyperparameter optimization, compute, little statistical or human insights, and the solution may not be robust Sanity checks are crucial to make sure that model’s assumptions are correct, preventing overfitting, ensuring integrity of the model Confounders ● Here, the confounding variable is temperature: high temperatures cause people to both eat more ice cream and spend more time outdoors under the sun, resulting in more sunburns. ● Confounders are correlated with the independent variable (may be causation) and causally related to the dependent variable ● Confounders bias results and prevent you from seeing actual relationships Bayes Rule ● Prior probability = p(Y) ○ Probability of case without information ● Posterior probability = p(Y|X) ○ Probability of case taking into account new information Independence ● If p(X,Y) = p(x)*p(y) or p(Y|X) = p(Y) Likelihood Function ● Maximum likelihood is an approach for determining the parameters in a probability distribution using an observed data set. Approach is to find parameter values that maximize the likelihood function ● We maximize the log of a function because it's more convenient - more mathematically simple and because the product of a large number of small probabilities can underflow the numerical precision of the computer but the sum of log probabilities wouldn't ● First maximize mean and then variance ● Over a sufficient data set the mean should equal the true mean but the variance estimate will underestimate by a factor of (N-1)/N - this is due to us maximizing the mean and then the variance, creating bias ○ Becomes less severe with higher N, but more severe with more parameters ● Negative log likelihood can be interpreted as a cost function similar to MSE for estimating the parameters of the distribution ● Likelihood refers to when you are trying to find the optimal value for the mean or standard deviation for a distribution given a bunch of observed measurements ○ Probability is the likelihood of observing measurements given a distribution Marginal distribution ● Probability distribution of a subset from a larger set of variables ○ Represents probabilities of\n",
      "\n",
      "Rank 2:\n",
      "  Distance: 0.2196\n",
      "  Video ID: AML Quiz 1 Study Guide.pdf\n",
      "  Frame: full_text\n",
      "  Timestamp: 0:00\n",
      "  Content: a one-unit change in X j all other variables fixed Collinearity ● Collinearity is a situation where 2 or more predictor variables in a multiple regression model are highly correlated ● Mostly effects interpretability, does not affect MSE or R^2 ○ When there is collinearity the predictors are not independent, which causes the coefficients to be less interpretable - changes in one variable are associated with the other which makes it hard to isolate the effect of individual predictors ● Increases uncertainty in coefficient estimates - small changes in the data can then lead to large fluctuations in the estimated values of the coefficients ● Dummy variables/one hot encoding ○ Must always do 1 less than the total amount ○ If you do create dummies for each category then you have perfect multicollinearity - sum of all dummies will always equal 1 for each observation. This linear dependency makes the model matrix singular , therefore the regression algorithm cannot invert the matrix to estimate the parameters. Ordinary Least Squares (OLS) ● Method used to estimate the parameters/weights of a regression model by minimizing the sum of squared errors (SSE) ● Loss function E(w) is the loss function OLS tries to minimize ○ ½ is for mathematical simplicity because you take the derivative and the squared cancels it out ● Minimizing MSE/SSE on the training data yields the Maximum likelihood estimate solution ○ Under the assumptions that the errors are normally distributed with constant variance (Batch Mode) Least Squares Solution ● Method for finding the optimal parameters (w) in a linear regression by using an exact closed form solution ● Uses matrix inversion on an (M+1) x (M+1) matrix where M is the number of basis functions (features). Φ 0 x = 1 which corresponds to the intercepts, where w 0 is the coefficient for the intercept ○ Can become computationally demanding as M increases ● Computation is linear in the size of the dataset (N) but cubic in the number of features (M) - more manageable to scale with more data points than with more features ● Method is batch mode which means it process the entire dataset at once to calculate the solution which is efficient for small or medium datasets but may not be for larger ones ● Collinearity problem is explicit because collinear variables makes the matrix difficult to invert, leading to high uncertainties in the parameter estimates Linear Least Squares vs Total Least Squares ● Linear least squares is used by OLS and minimizes the vertical distances between the observed data and regression line - only accounts for errors in the dependent variable and assumes the predictors are measured without error ○ Best for when error or noise is only found in the dependent variable ● Total least squares minimizes perpendicular distances from the data points to the regression line, which accounts for errors in both dependent and independent variables ○ Best for when there are measurement errors in both independent and dependent Model Complexity and\n",
      "\n",
      "Rank 3:\n",
      "  Distance: 0.2039\n",
      "  Video ID: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Frame: frame_700.jpg\n",
      "  Timestamp: 11:40\n",
      "  Content: Caption: the codel's book Text: def backprop(self, x, y):\n",
      "\n",
      "\"\"\"Return a tuple **(nabla_b, nabla_w)** representing the\n",
      "\n",
      "gradient for the cost function C_x. ~*‘nabla_b** and\n",
      "\n",
      "*“nabla_w** are layer-by-layer lists of numpy arrays, similar\n",
      "\n",
      "to ‘‘self.biases** and ‘*self.weights**.\"\"\"\n",
      "\n",
      "Nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
      "\n",
      "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
      "\n",
      "# feedforward\n",
      "\n",
      "activation = x\n",
      "\n",
      "activations = [x] # list to store all the activations, layer by layer\n",
      "\n",
      "zs = [] # list to store all the z vectors, layer by layer\n",
      "\n",
      "for b, w in zip(self.biases, self.weights):\n",
      "z = np.dot(w, activation)+b\n",
      "zs.append(z)\n",
      "activation = self.non_linearity(z)\n",
      "activations. append(activation)\n",
      "\n",
      "# backward pass\n",
      "\n",
      "delta = self.cost_derivative(activations[-1], y) * \\\n",
      "self.d_non_linearity(zs[-1])\n",
      "\n",
      "nabla_b[-1] = delta\n",
      "\n",
      "nabla_w[-1] = np.dot(delta, activations [-2].transpose())\n",
      "\n",
      "# Note that the variable l in the loop below is used a little\n",
      "\n",
      "# differently to the notation in Chapter 2 of the book. Here,\n",
      "\n",
      "# 1= 1 means the last layer of neurons, l = 2 is the\n",
      "\n",
      "# second-last layer, and so on. It's a renumbering of the\n",
      "\n",
      "# scheme in the book, used here to take advantage of the fact\n",
      "\n",
      "# that Python can use negative indices in lists.\n",
      "\n",
      "for lL in xrange(2, self.num_layers):\n",
      "sp si The code you'd find\n",
      "sp = self.d_non_linearity(z)\n",
      "delta = np.dot(self.weights[-1+1].transpose(), delta) * sp € CO € you nh\n",
      "nabla_b[-1] = delta . . )\n",
      "nabla_w[-1] = np.dot(delta, activations [-1-1].transpose()) in Nielsen S book\n",
      "\n",
      "return (nabla_b, nabla_w)\n",
      " Visual: web site, website, internet site, site\n",
      "\n",
      "Rank 4:\n",
      "  Distance: 0.1629\n",
      "  Video ID: AML Quiz 1 Study Guide.pdf\n",
      "  Frame: full_text\n",
      "  Timestamp: 0:00\n",
      "  Content: or noise is only found in the dependent variable ● Total least squares minimizes perpendicular distances from the data points to the regression line, which accounts for errors in both dependent and independent variables ○ Best for when there are measurement errors in both independent and dependent Model Complexity and Overfitting ● High complexity (more features) increases risk of overfitting, high N (more data points) reduces risk of overfitting with more complex models ● Regularization - impose penalties on less desirable solutions ○ Lambda is alpha in SKlearn ○ Regularization penalty (Penalty(f)) is a functional (maps the function of the regularization onto a number) which quantifies how complex or undesirable the solution is ○ Intercept is not penalized ○ Variables should be standardized so all features are on the same scale/magnitude otherwise features with large scale would dominate the penalty term ● Popular penalties ○ Ridge Regression - penalty is sum of squared weights ■ Discourages large weight values and shrinks them toward zero (never exactly zero) - also known as shrinkage or weight decay. Useful when we want to keep all features ■ Ridge regression has an exact closed form solution ○ Lasso Regression - adds penalty on the sum of the absolute values of the weights . ■ Can push some weights to exactly zero which performs feature selection ○ Elastic Net - combines both ridge and lasso penalties by shrinking weights and setting some weights to zero ○ Number of non-zero weights - leads weights to be driven to zero ○ Smoothness of function - penalize models that are not smooth (punishes I’m fluctuation) Kaggle data set issue ● Test set may be different if you sample the data again, meaning other models may perform better on other test sets - want to find how good the data would be on an infinite test set (true generalization error) Adjusted R^2 ● Adjusts for the difference between your training data and true data - so as the amount of training data increases, the difference can be expected to shrink\n",
      "\n",
      "Rank 5:\n",
      "  Distance: 0.1473\n",
      "  Video ID: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Frame: frame_345.jpg\n",
      "  Timestamp: 5:45\n",
      "  Content: Caption: a graph graph with a graph graph and a graph graph Text: 0.72\n",
      "—0.93\n",
      "-VCl...)=]\n",
      "All Cots re\n",
      "and biases °\n",
      "1.52\n",
      "\n",
      "C(wo, W1,--. ,W13,001) = 2.85\n",
      " Visual: oscilloscope, scope, cathode-ray oscilloscope, CRO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your query text\n",
    "query = \"What is confounder bias?\"\n",
    "\n",
    "results = search_index(query, model, index, content_items, k=5)\n",
    "\n",
    "for rank, (dist, item) in enumerate(results, start=1):\n",
    "    print(f\"Rank {rank}:\")\n",
    "    print(f\"  Distance: {dist:.4f}\")\n",
    "    print(f\"  Video ID: {item['video_id']}\")\n",
    "    print(f\"  Frame: {item['frame']}\")\n",
    "    print(f\"  Timestamp: {item['timestamp']}\")\n",
    "    print(f\"  Content: {item['content']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fefc16-2716-4be8-b53e-dd667d657f2f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0da45a-124f-4da2-98d2-b77e48a6d876",
   "metadata": {},
   "source": [
    "## Using a LLM To Implement the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46c744-c2c5-4e51-b5da-03dc45edafa4",
   "metadata": {},
   "source": [
    "### Using Gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a63d704-5289-4839-9ddd-eb0b1fe45362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667fd022-742e-48dc-8748-c0253d5296a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, retrieved_context: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the retrieved context and Gemini model.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query.\n",
    "        retrieved_context: List of tuples (score, content_item) from retrieval.\n",
    "        \n",
    "    Returns:\n",
    "        The generated answer as a string.\n",
    "    \"\"\"\n",
    "    # Combine retrieved content into a single context string\n",
    "    context_text = \"\\n\\n\".join([f\"{item['content']}\" for score, item in retrieved_context])\n",
    "    \n",
    "    # Construct the prompt by including the retrieved context and the user query\n",
    "    prompt = (\n",
    "        \"You are an expert tutor. Use the context provided below to answer the following question.\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Extract the answer from the response\n",
    "    answer = response.text.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b107f4f-681c-4935-a074-bc319890dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Gemini API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from getpass import getpass \n",
    "api_key = getpass(\"Enter your Gemini API key: \")\n",
    "genai.configure(api_key = api_key)\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "encoder_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ecef719-aad9-47c5-b7de-a0987b55f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " Confounder bias occurs when a confounding variable is correlated with the independent variable (potentially causally) and also causally related to the dependent variable.  This leads to biased results and prevents the observation of the true relationship between the independent and dependent variables.  The provided example uses ice cream consumption and sunburns, where temperature is the confounding variable.  High temperatures increase both ice cream consumption and sunburns, creating a spurious correlation between the two that masks the lack of a direct causal relationship.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is confounder bias?\"\n",
    "results = search_index(query, encoder_model, index, content_items, k=5)  # This comes from the FAISS Search\n",
    "answer = generate_answer(query, results)\n",
    "print(\"Generated Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74949c3-d4d3-45db-b051-5fd228d10a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
