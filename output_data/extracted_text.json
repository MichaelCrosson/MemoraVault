[
    {
        "video_id": "Ilg3gGewQ5U_transcript.docx",
        "frame": "full_text",
        "timestamp": "0:00",
        "timestamp_seconds": 0,
        "content": "YouTube Transcript - Ilg3gGewQ5U\nHere, we tackle backpropagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. We can't directly change those activations, we only have influence on the weights and biases. But it's helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about a 2. To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now of course, we cannot directly influence those activations, we only have control over the weights and biases. But just as with the last layer, it's helpful to keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup. If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get. But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with."
    },
    {
        "video_id": "AML Quiz 1 Study Guide.docx",
        "frame": "full_text",
        "timestamp": "0:00",
        "timestamp_seconds": 0,
        "content": "3 Types of Analytics\nDescriptive - find human interpretable patterns that describe the data\nLarge scale summaries, local patterns - finding fraud\nPredictive - use variables to predict unknown or future values of other variables\nRegression, classification, ranking/recommendation\nPrescriptive (may need causal reasoning)\n\nAnalysis is often retrospective (not prospective) data was not collected in a methodical way that is tailored for the analytical task\nData may be collected in the past without a specific analytical task in mind, which can lead to challenges because the data was not gathered with a methodology to ensure relevance or completeness for the analytics to be done - more risk for biases that are not controlled\n\nNo Free Lunch - there is no universally best model, so tradeoffs need to be understood\nDeep nets and complex models are very general and powerful (few to no assumptions about the nature of relationships) but require lots of data, hyperparameter optimization, compute, little statistical or human insights, and the solution may not be robust\n\nSanity checks are crucial to make sure that model’s assumptions are correct, preventing overfitting, ensuring integrity of the model\n\nConfounders\n\nHere, the confounding variable is temperature: high temperatures cause people to both eat more ice cream and spend more time outdoors under the sun, resulting in more sunburns.\nConfounders are correlated with the independent variable (may be causation) and causally related to the dependent variable\nConfounders bias results and prevent you from seeing actual relationships\n\nBayes Rule\n\nPrior probability = p(Y)\nProbability of case without information\nPosterior probability = p(Y|X)\nProbability of case taking into account new information\n\nIndependence\nIf p(X,Y) = p(x)*p(y) or p(Y|X) = p(Y)\n\nLikelihood Function\nMaximum likelihood is an approach for determining the parameters in a probability distribution using an observed data set. Approach is to find parameter values that maximize the likelihood function\nWe maximize the log of a function because it's more convenient - more mathematically simple and because the product of a large number of small probabilities can underflow the numerical precision of the computer but the sum of log probabilities wouldn't\nFirst maximize mean and then variance\nOver a sufficient data set the mean should equal the true mean but the variance estimate will underestimate by a factor of (N-1)/N - this is due to us maximizing the mean and then the variance, creating bias\nBecomes less severe with higher N, but more severe with more parameters\nNegative log likelihood can be interpreted as a cost function similar to MSE for estimating the parameters of the distribution\nLikelihood refers to when you are trying to find the optimal value for the mean or standard deviation for a distribution given a bunch of observed measurements\nProbability is the likelihood of observing measurements given a distribution\n\nMarginal distribution\nProbability distribution of a subset from a larger set of variables\nRepresents probabilities of the subset while ignoring or summing over the probabilities of the other variables\n\nConditional Distribution\nDistribution of one variable give something true about the other variable\nUsually in % - % of people who have a gpa Y given they are age 20\n\nCovariance\nStatistical measure that indicates the degree to which 2 random variables change together, does not provide information on the strength of the relationship (not normalized like correlation)\n\nMultiple Linear Regression - MLR\nStatistical format = \nCoefficients are betas, dependent variable is Y, estimates are indicated by hats\nLearned by Ordinary Least Squares OLS - hyperplane is fitted by minimizing the mean squared error MSE\nAlternative statistical format\n\nMeaning: Y|Dependents ~ Error terms\nY|Dependents: Conditional mean of Y is linear in the dependent variables\nError term:\nNormally distributed\nIndependent from each other\nIdentically distributed\nMinimizing MSE on the training data yields the maximum likelihood estimate (MLE) solution of the assumed generative model\nML Notation (Generalized linear regression)\nGeneralized linear regression is a linear combination of basis functions\n\ny(x,w) is the output (dependent)\nW is the vector of weights of coefficients\nϕi(x) are the basis functions which transform the input x\nM represents the number of basis functions \nParametric model - Assume a predetermined structure and a set of parameters(weights) to estimate using data\nSteps for learning a parametric model:\nDetermine the functional form of the model (polynomials, neural networks, linear functions) which are the mathematical structure that describes how the inputs relate to the outputs\nLearn the parameters (weights) using the training data. In linear models, parameters would be the coefficients/betas\nSpecial cases\nLinear regression - most basic case where the basis function ϕi(x) is simply the input variables themselves - no transformation\nPolynomial regression with scalar x - in this case the input x has polynomial terms (x, x2,x3) so the MLR model uses ϕi(x) = xi which allows the model to fit curves to the data\nInterpreting betas\n\nBeta is the partial derivative of the expected value of Y with respect to the predictor Xj, holding all other predictors constant - Bj is how much Y is expected to change for a one-unit change in Xj all other variables fixed\n\nCollinearity\nCollinearity is a situation where 2 or more predictor variables in a multiple regression model are highly correlated\nMostly effects interpretability, does not affect MSE or R^2\nWhen there is collinearity the predictors are not independent, which causes the coefficients to be less interpretable - changes in one variable are associated with the other which makes it hard to isolate the effect of individual predictors\nIncreases uncertainty in coefficient estimates - small changes in the data can then lead to large fluctuations in the estimated values of the coefficients\nDummy variables/one hot encoding\nMust always do 1 less than the total amount\nIf you do create dummies for each category then you have perfect multicollinearity - sum of all dummies will always equal 1 for each observation. This linear dependency makes the model matrix singular, therefore the regression algorithm cannot invert the matrix to estimate the parameters.\n\nOrdinary Least Squares (OLS)\nMethod used to estimate the parameters/weights of a regression model by minimizing the sum of squared errors (SSE)\nLoss function E(w) is the loss function OLS tries to minimize\n\n½ is for mathematical simplicity because you take the derivative and the squared cancels it out\nMinimizing MSE/SSE on the training data yields the Maximum likelihood estimate solution\nUnder the assumptions that the errors are normally distributed with constant variance\n\n(Batch Mode) Least Squares Solution\n\nMethod for finding the optimal parameters (w) in a linear regression by using an exact closed form solution\nUses matrix inversion on an (M+1) x (M+1) matrix where M is the number of basis functions (features). Φ0x = 1 which corresponds to the intercepts, where w0 is the coefficient for the intercept\nCan become computationally demanding as M increases\nComputation is linear in the size of the dataset (N) but cubic in the number of features (M) - more manageable to scale with more data points than with more features\nMethod is batch mode which means it process the entire dataset at once to calculate the solution which is efficient for small or medium datasets but may not be for larger ones\nCollinearity problem is explicit because collinear variables makes the matrix difficult to invert, leading to high uncertainties in the parameter estimates\n\nLinear Least Squares vs Total Least Squares\n\nLinear least squares is used by OLS and minimizes the vertical distances between the observed data and regression line - only accounts for errors in the dependent variable and assumes the predictors are measured without error\nBest for when error or noise is only found in the dependent variable\nTotal least squares minimizes perpendicular distances from the data points to the regression line, which accounts for errors in both dependent and independent variables\nBest for when there are measurement errors in both independent and dependent\n\nModel Complexity and Overfitting\nHigh complexity (more features) increases risk of overfitting, high N (more data points) reduces risk of overfitting with more complex models\nRegularization - impose penalties on less desirable solutions\n\nLambda is alpha in SKlearn\nRegularization penalty (Penalty(f)) is a functional (maps the function of the regularization onto a number) which quantifies how complex or undesirable the solution is\nIntercept is not penalized\nVariables should be standardized so all features are on the same scale/magnitude otherwise features with large scale would dominate the penalty term\nPopular penalties\nRidge Regression - penalty is sum of squared weights\nDiscourages large weight values and shrinks them toward zero (never exactly zero) - also known as shrinkage or weight decay. Useful when we want to keep all features\nRidge regression has an exact closed form solution\nLasso Regression - adds penalty on the sum of the absolute values of the weights. \nCan push some weights to exactly zero which performs feature selection\nElastic Net - combines both ridge and lasso penalties by shrinking weights and setting some weights to zero\nNumber of non-zero weights - leads weights to be driven to zero\nSmoothness of function - penalize models that are not smooth (punishes I’m fluctuation)\nKaggle data set issue\nTest set may be different if you sample the data again, meaning other models may perform better on other test sets - want to find how good the data would be on an infinite test set (true generalization error)\n\nAdjusted R^2\nAdjusts for the difference between your training data and true data - so as the amount of training data increases, the difference can be expected to shrink"
    },
    {
        "video_id": "AML Quiz 1 Study Guide.pdf",
        "frame": "full_text",
        "timestamp": "0:00",
        "timestamp_seconds": 0,
        "content": "3 Types of Analytics 1. Descriptive - find human interpretable patterns that describe the data a. Large scale summaries, local patterns - finding fraud 2. Predictive - use variables to predict unknown or future values of other variables a. Regression, classification, ranking/recommendation 3. Prescriptive (may need causal reasoning) Analysis is often retrospective (not prospective) data was not collected in a methodical way that is tailored for the analytical task ● Data may be collected in the past without a specific analytical task in mind, which can lead to challenges because the data was not gathered with a methodology to ensure relevance or completeness for the analytics to be done - more risk for biases that are not controlled No Free Lunch - there is no universally best model, so tradeoffs need to be understood ● Deep nets and complex models are very general and powerful (few to no assumptions about the nature of relationships) but require lots of data, hyperparameter optimization, compute, little statistical or human insights, and the solution may not be robust Sanity checks are crucial to make sure that model’s assumptions are correct, preventing overfitting, ensuring integrity of the model Confounders ● Here, the confounding variable is temperature: high temperatures cause people to both eat more ice cream and spend more time outdoors under the sun, resulting in more sunburns. ● Confounders are correlated with the independent variable (may be causation) and causally related to the dependent variable\n● Confounders bias results and prevent you from seeing actual relationships Bayes Rule ● Prior probability = p(Y) ○ Probability of case without information ● Posterior probability = p(Y|X) ○ Probability of case taking into account new information Independence ● If p(X,Y) = p(x)*p(y) or p(Y|X) = p(Y) Likelihood Function ● Maximum likelihood is an approach for determining the parameters in a probability distribution using an observed data set. Approach is to find parameter values that maximize the likelihood function ● We maximize the log of a function because it's more convenient - more mathematically simple and because the product of a large number of small probabilities can underflow the numerical precision of the computer but the sum of log probabilities wouldn't ● First maximize mean and then variance ● Over a sufficient data set the mean should equal the true mean but the variance estimate will underestimate by a factor of (N-1)/N - this is due to us maximizing the mean and then the variance, creating bias ○ Becomes less severe with higher N, but more severe with more parameters ● Negative log likelihood can be interpreted as a cost function similar to MSE for estimating the parameters of the distribution ● Likelihood refers to when you are trying to find the optimal value for the mean or standard deviation for a distribution given a bunch of observed measurements ○ Probability is the likelihood of observing measurements given a distribution Marginal distribution ● Probability distribution of a subset from a larger set of variables ○ Represents probabilities of the subset while ignoring or summing over the probabilities of the other variables Conditional Distribution ● Distribution of one variable give something true about the other variable ○ Usually in % - % of people who have a gpa Y given they are age 20 Covariance\n● Statistical measure that indicates the degree to which 2 random variables change together, does not provide information on the strength of the relationship (not normalized like correlation) Multiple Linear Regression - MLR ● Statistical format = ○ Coefficients are betas, dependent variable is Y, estimates are indicated by hats ○ Learned by Ordinary Least Squares OLS - hyperplane is fitted by minimizing the mean squared error MSE ● Alternative statistical format ○ ○ Meaning: Y|Dependents ~ Error terms ■ Y|Dependents: Conditional mean of Y is linear in the dependent variables ■ Error term: ● Normally distributed ● Independent from each other ● Identically distributed ○ Minimizing MSE on the training data yields the maximum likelihood estimate (MLE) solution of the assumed generative model ● ML Notation (Generalized linear regression) ○ Generalized linear regression is a linear combination of basis functions ■ y(x,w) is the output (dependent) ■ W is the vector of weights of coefficients ■ ϕ i (x) are the basis functions which transform the input x ■ M represents the number of basis functions ○ Parametric model - Assume a predetermined structure and a set of parameters(weights) to estimate using data ○ Steps for learning a parametric model: ■ Determine the functional form of the model (polynomials, neural networks, linear functions) which are the mathematical structure that describes how the inputs relate to the outputs ■ Learn the parameters (weights) using the training data. In linear models, parameters would be the coefficients/betas\n○ Special cases ■ Linear regression - most basic case where the basis function ϕ i (x) is simply the input variables themselves - no transformation ■ Polynomial regression with scalar x - in this case the input x has polynomial terms (x, x 2 ,x 3 ) so the MLR model uses ϕ i (x) = x i which allows the model to fit curves to the data ● Interpreting betas ○ Beta is the partial derivative of the expected value of Y with respect to the predictor X j , holding all other predictors constant - B j is how much Y is expected to change for a one-unit change in X j all other variables fixed Collinearity ● Collinearity is a situation where 2 or more predictor variables in a multiple regression model are highly correlated ● Mostly effects interpretability, does not affect MSE or R^2 ○ When there is collinearity the predictors are not independent, which causes the coefficients to be less interpretable - changes in one variable are associated with the other which makes it hard to isolate the effect of individual predictors ● Increases uncertainty in coefficient estimates - small changes in the data can then lead to large fluctuations in the estimated values of the coefficients ● Dummy variables/one hot encoding ○ Must always do 1 less than the total amount ○ If you do create dummies for each category then you have perfect multicollinearity - sum of all dummies will always equal 1 for each observation. This linear dependency makes the model matrix singular , therefore the regression algorithm cannot invert the matrix to estimate the parameters. Ordinary Least Squares (OLS) ● Method used to estimate the parameters/weights of a regression model by minimizing the sum of squared errors (SSE)\n● Loss function E(w) is the loss function OLS tries to minimize ○ ½ is for mathematical simplicity because you take the derivative and the squared cancels it out ● Minimizing MSE/SSE on the training data yields the Maximum likelihood estimate solution ○ Under the assumptions that the errors are normally distributed with constant variance (Batch Mode) Least Squares Solution ● Method for finding the optimal parameters (w) in a linear regression by using an exact closed form solution ● Uses matrix inversion on an (M+1) x (M+1) matrix where M is the number of basis functions (features). Φ 0 x = 1 which corresponds to the intercepts, where w 0 is the coefficient for the intercept ○ Can become computationally demanding as M increases ● Computation is linear in the size of the dataset (N) but cubic in the number of features (M) - more manageable to scale with more data points than with more features ● Method is batch mode which means it process the entire dataset at once to calculate the solution which is efficient for small or medium datasets but may not be for larger ones ● Collinearity problem is explicit because collinear variables makes the matrix difficult to invert, leading to high uncertainties in the parameter estimates Linear Least Squares vs Total Least Squares\n● Linear least squares is used by OLS and minimizes the vertical distances between the observed data and regression line - only accounts for errors in the dependent variable and assumes the predictors are measured without error ○ Best for when error or noise is only found in the dependent variable ● Total least squares minimizes perpendicular distances from the data points to the regression line, which accounts for errors in both dependent and independent variables ○ Best for when there are measurement errors in both independent and dependent Model Complexity and Overfitting ● High complexity (more features) increases risk of overfitting, high N (more data points) reduces risk of overfitting with more complex models ● Regularization - impose penalties on less desirable solutions ○ Lambda is alpha in SKlearn ○ Regularization penalty (Penalty(f)) is a functional (maps the function of the regularization onto a number) which quantifies how complex or undesirable the solution is ○ Intercept is not penalized\n○ Variables should be standardized so all features are on the same scale/magnitude otherwise features with large scale would dominate the penalty term ● Popular penalties ○ Ridge Regression - penalty is sum of squared weights ■ Discourages large weight values and shrinks them toward zero (never exactly zero) - also known as shrinkage or weight decay. Useful when we want to keep all features ■ Ridge regression has an exact closed form solution ○ Lasso Regression - adds penalty on the sum of the absolute values of the weights . ■ Can push some weights to exactly zero which performs feature selection ○ Elastic Net - combines both ridge and lasso penalties by shrinking weights and setting some weights to zero ○ Number of non-zero weights - leads weights to be driven to zero ○ Smoothness of function - penalize models that are not smooth (punishes I’m fluctuation) Kaggle data set issue ● Test set may be different if you sample the data again, meaning other models may perform better on other test sets - want to find how good the data would be on an infinite test set (true generalization error) Adjusted R^2 ● Adjusts for the difference between your training data and true data - so as the amount of training data increases, the difference can be expected to shrink\n"
    },
    {
        "video_id": "Ilg3gGewQ5U_transcript.docx",
        "frame": "full_text",
        "timestamp": "0:00",
        "timestamp_seconds": 0,
        "content": "YouTube Transcript - Ilg3gGewQ5U\nHere, we tackle backpropagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. We can't directly change those activations, we only have influence on the weights and biases. But it's helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about a 2. To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now of course, we cannot directly influence those activations, we only have control over the weights and biases. But just as with the last layer, it's helpful to keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup. If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get. But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with."
    }
]