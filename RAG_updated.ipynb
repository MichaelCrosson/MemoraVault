{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cb6731-d654-4fad-9446-d358c0e511f2",
   "metadata": {},
   "source": [
    "## The following RAG notebook covers the following: \n",
    "### 1. Vectorizing all the content being fetched (Data Ingestion)\n",
    "### 2. Storing all the embeddings into a vector database and retrieving it (Retrieval)\n",
    "### 3. Querying the LLM (Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f05a66-a557-4f47-b976-6d6d566b0a31",
   "metadata": {},
   "source": [
    "## Part 1: Vectorization of all the content being fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1c82f-5bbf-4a0a-9616-b2e6b0fbc9ce",
   "metadata": {},
   "source": [
    "### Loading and preparing the JSON Data (Data Ingestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8de0a-067d-467f-9d80-3bfb40f0bc4b",
   "metadata": {},
   "source": [
    "Install the below dependencies first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1145356-445a-4b04-b531-4feba0115bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0647dbe-01ad-4e01-861d-e3b352c3c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fdc9e3-7937-4b3f-84da-4d486b1a825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Unstructured/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits the input text into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to split.\n",
    "        chunk_size: Maximum number of words per chunk.\n",
    "        overlap: Number of overlapping words between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "def load_json_files(json_folder: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all JSON files from a folder and extract their content.\n",
    "    \n",
    "    Args:\n",
    "        json_folder: Path to folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with extracted content.\n",
    "    \"\"\"\n",
    "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "    all_content = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        video_id = os.path.basename(json_file).replace(\"_processed.json\", \"\")\n",
    "        print(f\"Processing: {json_file}\")\n",
    "        \n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Process each frame in the JSON\n",
    "            for i, frame in enumerate(data):\n",
    "                frame_name = frame.get(\"frame\", f\"frame_{i}\")\n",
    "                text_content = []\n",
    "                \n",
    "                if frame.get(\"caption\"):\n",
    "                    text_content.append(f\"Caption: {frame['caption']}\")\n",
    "                if frame.get(\"extracted_text\"):\n",
    "                    text_content.append(f\"Text: {frame['extracted_text']}\")\n",
    "                if frame.get(\"label\"):\n",
    "                    text_content.append(f\"Visual: {frame['label']}\")\n",
    "                \n",
    "                if text_content:\n",
    "                    frame_time = 0\n",
    "                    try:\n",
    "                        time_str = frame_name.replace(\"frame_\", \"\").replace(\".jpg\", \"\")\n",
    "                        frame_time = int(time_str)\n",
    "                    except:\n",
    "                        frame_time = i * 5\n",
    "                    \n",
    "                    time_min = frame_time // 60\n",
    "                    time_sec = frame_time % 60\n",
    "                    time_str = f\"{time_min}:{time_sec:02d}\"\n",
    "                    \n",
    "                    content_item = {\n",
    "                        \"video_id\": video_id,\n",
    "                        \"frame\": frame_name,\n",
    "                        \"timestamp\": time_str,\n",
    "                        \"timestamp_seconds\": frame_time,\n",
    "                        \"content\": \" \".join(text_content)\n",
    "                    }\n",
    "                    \n",
    "                    all_content.append(content_item)\n",
    "            \n",
    "            print(f\"  Extracted {len(all_content)} content items so far\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {e}\")\n",
    "    \n",
    "    return all_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559638e-208a-4acd-981b-6f4ff2884c80",
   "metadata": {},
   "source": [
    "### Conversion of .json files into embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcdb993-b725-4bd5-b6c7-7adde75d4270",
   "metadata": {},
   "source": [
    "We are using the all-miniLM-l6-v2 model for vectorizing the json files into embeddings. \n",
    "\n",
    "The hugging face link: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90043983-46ad-4e03-b543-998aa631539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_content(content_items: List[Dict[str, Any]],\n",
    "                      model_name: str = \"all-MiniLM-L6-v2\",\n",
    "                      chunk_size: int = 500,\n",
    "                      overlap: int = 50) -> Tuple[np.ndarray, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Convert content items to vector embeddings. For long texts, perform chunking.\n",
    "    \n",
    "    Args:\n",
    "        content_items: List of content items with text.\n",
    "        model_name: Name of the SentenceTransformer model to use.\n",
    "        chunk_size: Maximum number of words in each chunk.\n",
    "        overlap: Number of overlapping words between chunks.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (embeddings array, updated content items with chunk info).\n",
    "    \"\"\"\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"Loaded embedding model: {model_name}\")\n",
    "    \n",
    "    chunked_items = []\n",
    "    for item in content_items:\n",
    "        text = item[\"content\"]\n",
    "        # Chuking\n",
    "        if len(text.split()) > chunk_size:\n",
    "            chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                new_item = item.copy()\n",
    "                new_item[\"content\"] = chunk\n",
    "                new_item[\"chunk_index\"] = i\n",
    "                chunked_items.append(new_item)\n",
    "        else:\n",
    "            chunked_items.append(item)\n",
    "    \n",
    "    texts = [item[\"content\"] for item in chunked_items]\n",
    "    print(f\"Generating embeddings for {len(texts)} chunks/items...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return embeddings, chunked_items\n",
    "\n",
    "def save_vectors(embeddings: np.ndarray, content_items: List[Dict[str, Any]], output_folder: str):\n",
    "    \"\"\"\n",
    "    Save the vector embeddings and content items.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings.\n",
    "        content_items: List of content items.\n",
    "        output_folder: Folder to save the files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    embeddings_path = os.path.join(output_folder, \"embeddings.npy\")\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    \n",
    "    content_path = os.path.join(output_folder, \"content_items.pkl\")\n",
    "    with open(content_path, 'wb') as f:\n",
    "        pickle.dump(content_items, f)\n",
    "    \n",
    "    print(f\"Saved embeddings to {embeddings_path}\")\n",
    "    print(f\"Saved content items to {content_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec91e5f-470c-46a5-a96d-0faa0e5618c9",
   "metadata": {},
   "source": [
    "### Main Function call for chunking, vectorizing data and storing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e8b6cb-5c81-4f1f-b0d8-ceb0afff9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/Data to be considered/Ilg3gGewQ5U_processed_filtered.json\n",
      "  Extracted 38 content items so far\n",
      "Extracted 38 total content items from all JSON files\n",
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Generating embeddings for 38 chunks/items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (38, 384)\n",
      "Saved embeddings to /Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/vectorized_data/embeddings.npy\n",
      "Saved content items to /Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/vectorized_data/content_items.pkl\n",
      "Vectorization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration folders\n",
    "    json_folder = '/Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/Data to be considered'\n",
    "    output_folder = '/Users/advaith/Desktop/MSBA Related coursework/Spring term/Deep Learning/Final Project/vectorized_data' \n",
    "    \n",
    "    content_items = load_json_files(json_folder)\n",
    "    print(f\"Extracted {len(content_items)} total content items from all JSON files\")\n",
    "    \n",
    "    embeddings, content_items = vectorize_content(content_items)\n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    save_vectors(embeddings, content_items, output_folder)\n",
    "    \n",
    "    print(\"Vectorization complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b802451-a71f-4d08-a67f-220dea0a4dee",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b3cbb-aa6c-443d-90c9-52396532b5e1",
   "metadata": {},
   "source": [
    "## Part 2: Storing all the embeddings into a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b129f-3c7b-4fd2-a282-a303cf5709d7",
   "metadata": {},
   "source": [
    "We will use the FAISS for searching embeddings\n",
    "\n",
    "Link for FAISS : https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451d3ac7-75a0-4f37-a0c4-26be9b908b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef46112-1a16-4dad-9463-a50b2316e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings with shape: (38, 384)\n",
      "Loaded 38 content items.\n",
      "FAISS index has 38 vectors.\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import glob\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def load_vectorized_data(embeddings_path: str, content_path: str):\n",
    "    \"\"\"\n",
    "    Load the saved embeddings (NumPy array) and content items (pickle file).\n",
    "    \"\"\"\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    with open(content_path, 'rb') as f:\n",
    "        content_items = pickle.load(f)\n",
    "    return embeddings, content_items\n",
    "\n",
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Create a FAISS index using cosine similarity (normalize + inner product).\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.astype(\"float32\")\n",
    "    \n",
    "    # Normalizing vectors for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "    index.add(embeddings)\n",
    "    print(f\"FAISS index has {index.ntotal} vectors.\")\n",
    "    return index\n",
    "\n",
    "def search_index(query: str, model: SentenceTransformer, index: faiss.Index, content_items: list, k: int = 5):\n",
    "    \"\"\"\n",
    "    Convert the query to an embedding, search the FAISS index for the top-k nearest neighbors,\n",
    "    and return the corresponding content items with their scores.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(query, show_progress_bar=False)\n",
    "    query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "    \n",
    "    \n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append((score, content_items[idx]))\n",
    "    return results\n",
    "\n",
    "\n",
    "## Change below accordingly \n",
    "\n",
    "embeddings_path = \"vectorized_data/embeddings.npy\"\n",
    "content_path = \"vectorized_data/content_items.pkl\"\n",
    "\n",
    "# Load vectorized data\n",
    "embeddings, content_items = load_vectorized_data(embeddings_path, content_path)\n",
    "print(f\"Loaded embeddings with shape: {embeddings.shape}\")\n",
    "print(f\"Loaded {len(content_items)} content items.\")\n",
    "\n",
    "# Create the FAISS index\n",
    "index = create_faiss_index(embeddings)\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0dd46-f4f0-4b54-b998-97f737d0a8a4",
   "metadata": {},
   "source": [
    "### Testing of the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fac556-1d0f-4cbc-8b7e-0a21aef85bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "  Distance: 0.4931\n",
      "  Video ID: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Frame: frame_700.jpg\n",
      "  Timestamp: 11:40\n",
      "  Content: Caption: the codel's book Text: def backprop(self, x, y):\n",
      "\n",
      "\"\"\"Return a tuple **(nabla_b, nabla_w)** representing the\n",
      "\n",
      "gradient for the cost function C_x. ~*‘nabla_b** and\n",
      "\n",
      "*“nabla_w** are layer-by-layer lists of numpy arrays, similar\n",
      "\n",
      "to ‘‘self.biases** and ‘*self.weights**.\"\"\"\n",
      "\n",
      "Nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
      "\n",
      "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
      "\n",
      "# feedforward\n",
      "\n",
      "activation = x\n",
      "\n",
      "activations = [x] # list to store all the activations, layer by layer\n",
      "\n",
      "zs = [] # list to store all the z vectors, layer by layer\n",
      "\n",
      "for b, w in zip(self.biases, self.weights):\n",
      "z = np.dot(w, activation)+b\n",
      "zs.append(z)\n",
      "activation = self.non_linearity(z)\n",
      "activations. append(activation)\n",
      "\n",
      "# backward pass\n",
      "\n",
      "delta = self.cost_derivative(activations[-1], y) * \\\n",
      "self.d_non_linearity(zs[-1])\n",
      "\n",
      "nabla_b[-1] = delta\n",
      "\n",
      "nabla_w[-1] = np.dot(delta, activations [-2].transpose())\n",
      "\n",
      "# Note that the variable l in the loop below is used a little\n",
      "\n",
      "# differently to the notation in Chapter 2 of the book. Here,\n",
      "\n",
      "# 1= 1 means the last layer of neurons, l = 2 is the\n",
      "\n",
      "# second-last layer, and so on. It's a renumbering of the\n",
      "\n",
      "# scheme in the book, used here to take advantage of the fact\n",
      "\n",
      "# that Python can use negative indices in lists.\n",
      "\n",
      "for lL in xrange(2, self.num_layers):\n",
      "sp si The code you'd find\n",
      "sp = self.d_non_linearity(z)\n",
      "delta = np.dot(self.weights[-1+1].transpose(), delta) * sp € CO € you nh\n",
      "nabla_b[-1] = delta . . )\n",
      "nabla_w[-1] = np.dot(delta, activations [-1-1].transpose()) in Nielsen S book\n",
      "\n",
      "return (nabla_b, nabla_w)\n",
      " Visual: web site, website, internet site, site\n",
      "\n",
      "Rank 2:\n",
      "  Distance: 0.4135\n",
      "  Video ID: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Frame: frame_640.jpg\n",
      "  Timestamp: 10:40\n",
      "  Content: Caption: the evolution of the evolution of the evolution of the evolution of the evolution of the evolution of the Text: Backpropagation\n",
      "\n",
      "titmt WN\n",
      " Visual: hair slide\n",
      "\n",
      "Rank 3:\n",
      "  Distance: 0.3003\n",
      "  Video ID: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Frame: frame_635.jpg\n",
      "  Timestamp: 10:35\n",
      "  Content: Caption: stochic descent / stochic descent / stochic descent / stochic descent Text: (/Stochastic gradient descent)\n",
      " Visual: matchstick\n",
      "\n",
      "Rank 4:\n",
      "  Distance: 0.2990\n",
      "  Video ID: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Frame: frame_605.jpg\n",
      "  Timestamp: 10:05\n",
      "  Content: Caption: a keyboard with numbers and numbers on it Text: OU STANZA 4H 3\n",
      "\n",
      "G\\72 Ff CF MOY\n",
      "\n",
      "Compute gradient descent step (using backprop)\n",
      "\n",
      "60361 8 7\n",
      "\n",
      "293273 F 6 7/0\n",
      "4349 8\n",
      "\n",
      " Visual: web site, website, internet site, site\n",
      "\n",
      "Rank 5:\n",
      "  Distance: 0.2840\n",
      "  Video ID: Ilg3gGewQ5U_processed_filtered.json\n",
      "  Frame: frame_600.jpg\n",
      "  Timestamp: 10:00\n",
      "  Content: Caption: a set of numbers with the same numbers Text: Compute gradient descent step\n",
      "\n",
      "aN\n",
      "\n",
      "using backprop)\n",
      "\n",
      "SIO 7 TA VF I\n",
      "Se1.7AF CSUN OY\n",
      "AZY3 272% °F 670\n",
      "60360187793\n",
      "\n",
      " Visual: typewriter keyboard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your query text\n",
    "query = \"How does backpropagation work?\"\n",
    "\n",
    "results = search_index(query, model, index, content_items, k=5)\n",
    "\n",
    "for rank, (dist, item) in enumerate(results, start=1):\n",
    "    print(f\"Rank {rank}:\")\n",
    "    print(f\"  Distance: {dist:.4f}\")\n",
    "    print(f\"  Video ID: {item['video_id']}\")\n",
    "    print(f\"  Frame: {item['frame']}\")\n",
    "    print(f\"  Timestamp: {item['timestamp']}\")\n",
    "    print(f\"  Content: {item['content']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fefc16-2716-4be8-b53e-dd667d657f2f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0da45a-124f-4da2-98d2-b77e48a6d876",
   "metadata": {},
   "source": [
    "## Using a LLM To Implement the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46c744-c2c5-4e51-b5da-03dc45edafa4",
   "metadata": {},
   "source": [
    "### Using Gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a63d704-5289-4839-9ddd-eb0b1fe45362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667fd022-742e-48dc-8748-c0253d5296a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, retrieved_context: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the retrieved context and Gemini model.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query.\n",
    "        retrieved_context: List of tuples (score, content_item) from retrieval.\n",
    "        \n",
    "    Returns:\n",
    "        The generated answer as a string.\n",
    "    \"\"\"\n",
    "    # Combine retrieved content into a single context string\n",
    "    context_text = \"\\n\\n\".join([f\"{item['content']}\" for score, item in retrieved_context])\n",
    "    \n",
    "    # Construct the prompt by including the retrieved context and the user query\n",
    "    prompt = (\n",
    "        \"You are an expert tutor. Use the context provided below to answer the following question.\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Extract the answer from the response\n",
    "    answer = response.text.strip()\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b107f4f-681c-4935-a074-bc319890dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Gemini API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from getpass import getpass \n",
    "api_key = getpass(\"Enter your Gemini API key: \")\n",
    "genai.configure(api_key = api_key)\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "encoder_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ecef719-aad9-47c5-b7de-a0987b55f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " Backpropagation calculates the gradient of the cost function with respect to the network's weights and biases.  The process begins with a feedforward pass, computing the activations (outputs) of each layer and storing them.  Then, a backward pass commences.  It starts at the output layer, calculating the error (delta) using the cost derivative and the derivative of the activation function. This error is then propagated back layer by layer.  For each layer, the gradient of the biases is simply the error (delta) at that layer.  The gradient of the weights is the product of the error (delta) and the previous layer's activations, appropriately transposed. This process iteratively uses the weights from the next layer and the derivative of the activation function to compute the error signal for the previous layer.  The algorithm leverages the chain rule of calculus to efficiently compute these gradients.  The final output is a pair of layer-by-layer lists: `nabla_b` (gradients for biases) and `nabla_w` (gradients for weights). These gradients are then used in gradient descent to update the network's weights and biases, minimizing the cost function.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does backpropagation work?\"\n",
    "results = search_index(query, encoder_model, index, content_items, k=5)  # This comes from the FAISS Search\n",
    "answer = generate_answer(query, results)\n",
    "print(\"Generated Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74949c3-d4d3-45db-b051-5fd228d10a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
